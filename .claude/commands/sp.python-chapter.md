---
description: Intelligence-driven workflow for Python chapters (12-29). Reads constitution + chapter-index to derive audience/complexity/prerequisites automatically. Asks only targeted questions when genuinely ambiguous. Chains /sp.specify â†’ /sp.plan â†’ /sp.tasks â†’ /sp.implement with validation gates.
---

# /sp.python-chapter: Intelligence-Driven Python Chapter Workflow

**Purpose**: Design a complete Python chapter (12-29) using **vertical intelligence** (constitution + chapter-index + skills) to automatically derive context. No hardcoded questions - the command reads authoritative sources and asks only what's genuinely ambiguous. Chains full workflow (Spec â†’ Plan â†’ Tasks â†’ Implement â†’ Validate) with approval gates.

**Intelligence Sources**:
- Constitution: Target audience, philosophy, learning patterns
- Chapter Index: Exact title (THE ANCHOR), part, prerequisites
- Skills Library: Available domain skills for this chapter
- Context Materials: Existing pedagogical patterns (if available)

**Adaptive Questions**: 0-3 targeted questions based on what intelligence can't derive.

## User Input

```text
$ARGUMENTS
```

## VERTICAL INTELLIGENCE: AIDD-Driven Python Teaching

Before orchestration begins, understand what makes Python chapters effective in the AI-native era:

### Core Principle: Specification-First, Validation-First, AI Partnership

Students don't memorize Python syntax. Instead:

1. **Understand the concept** (plain language explanation)
2. **See minimal code** (what it does in action)
3. **Ask their AI** (explore through dialogue with Claude Code/Gemini CLI)
4. **Extract insight** (why this matters for thinking, not just coding)

### AI-Native Learning for Part 4 Students

**Traditional Programming Teaching**:
- "Memorize Python syntax"
- "Here are all 47 string methods"
- Syntax-first (memorize, then apply)

**AI-Native Learning Pattern** (Part 4: Chapters 12-29):
- **Describe Intent**: Use type hints and clear code to communicate what data means
- **Explore with AI**: Ask AI questions to understand concepts (not memorize docs)
- **Validate Together**: Use isinstance(), type(), and tests to check understanding
- **Learn from Errors**: When errors occur, ask AI "why?" and learn the pattern

**Note on AIDD**: Students in Chapters 1-11 learned AIDD principles. Part 4 applies these principles to learning Python, using the beginner-friendly "AI-Native Learning" framing. Students don't write formal specifications yet (that's Part 5+), but they DO describe intent through type hints and clear code structure.

### Teaching Pattern (Every Concept)

```markdown
## 1. [Concept Name] â€” [Why it matters]

**What it is:**
Plain-language explanation (2-3 sentences).

### ğŸ’» Code Idea

\`\`\`python
# Minimal code showing the concept
# Focus on WHAT it does
\`\`\`

### ğŸ¤– Think With Your AI

> "What does this do?"
>
> "What changes if we...?"
>
> "How would you use this to...?"

### ğŸ§  The Reasoning Pattern

[Why this concept matters for thinking, not just coding]
```

**Example:**

```markdown
## 1. Variables â€” Storing Data

**What it is:**
A variable names a value so your program can remember it.

### ğŸ’» Code Idea

\`\`\`python
name = "Alex"
score = 95
\`\`\`

### ğŸ¤– Think With Your AI

> "Why do we need variables instead of just using 95?"
>
> "What breaks if we forget to name a value?"
>
> "How do AI agents use variables to track context?"

### ğŸ§  The Reasoning Pattern

Programs need memory. Variables let you say "remember this as X"â€”
exactly how reasoning chains in AI maintain state.
```

---

## Python Standards (Chapters 12-29)

**Version:** 3.14+ (always use latest stable release from https://www.python.org/downloads/)
**Syntax:** f-strings only, match/case (17+), modern types (`list[int]`, `X | None`)
**Type hints:** Core (Ch 13) â†’ Gradual Application (14-26) â†’ Mandatory (27+)
**Note on Type Hints:** Modern Python treats type hints as essential for clarity and specification-first thinking, not optional features. Integrate from Chapter 13 onwards.
**MCP Documentation Source**: Python.org official docs via context7 MCP server (loads current, authoritative reference)

**Security (non-negotiable):**
- âŒ No `eval()`, `shell=True`, hardcoded secrets
- âœ… Environment variables, input validation, modern patterns

---

## CRITICAL DESIGN RULES

### Rule 1: USER INTENT IS AUTHORITY

**Never override user input:**
- User says "beginner" â†’ Make A1-A2 (NOT A2-B1)
- User says "just variables" â†’ Only variables (NOT + functions + loops)
- User says "absolute beginners" â†’ 5 concepts max, simple framing

**Always ask, always honor. Do NOT assume.**

---

### Rule 2: NO FORWARD REFERENCES + PART 4 APPROPRIATE LANGUAGE

**Never mention untaught concepts:**
- âŒ NO Chapter 30+ references
- âŒ NO "Specification-Driven Development" (not yet taught - that's Part 5+)
- âŒ NO "write a specification" (use "describe your intent" instead)
- âŒ NO professional SDD terminology for Part 4 students

**DO reference AI-Native Learning (appropriate for Part 4):**
- âœ… "Describe what your code should do using type hints..."
- âœ… "Ask your AI to explain this concept..."
- âœ… "Validate your understanding by testing the code..."
- âœ… "Learn from errors by asking AI 'why did this fail?'..."

**DO reference AIDD principles from Chapters 1-11 (context only):**
- âœ… "This applies the AIDD thinking you learned in Part 1..."
- âœ… "Remember the validation-first approach from Chapter 4..."
- âœ… "You're using AI as co-reasoning partner, not coding assistant..."

**Critical Distinction**:
- Part 4 students use **AI-Native Learning** (beginner-friendly: describe intent â†’ explore â†’ validate â†’ learn from errors)
- Part 5+ students learn **Specification-Driven Development** (professional: write formal specs â†’ generate â†’ test â†’ iterate)
- Type hints are "describing intent" NOT "writing specifications" in Part 4

---

### Rule 3: RUTHLESS CONTEXT FILTERING

**When extracting from context materials:**

**Chapter 13 "Introduction to Python":**
- âœ… "What is Python?" â†’ USE (intro concept)
- âœ… "Your first program" â†’ USE (intro outcome)
- âŒ "Functions" â†’ SKIP (Ch 20 topic)
- âŒ "Classes" â†’ SKIP (Ch 24+ topic)
- âŒ "Async/await" â†’ SKIP (Ch 28 topic)

**Chapter 17 "Control Flow and Loops":**
- âœ… "if/elif/else statements" â†’ USE (chapter focus)
- âœ… "for loops" â†’ USE (chapter focus)
- âŒ "Functions" â†’ SKIP (Ch 20 topic)
- âŒ "List comprehensions" â†’ SKIP (advanced)
- âŒ "Exception handling" â†’ SKIP (Ch 21 topic)

**Decision Rule:**
- IF context concept fits THIS chapter's title â†’ EXTRACT
- IF context concept belongs to Ch N+1 or later â†’ SKIP
- IF context concept is advanced variation â†’ SKIP
- IF context concept requires future prerequisites â†’ SKIP

---

### Rule 4: MINIMAL SCOPE

**Depth > breadth.**

- Beginner (Ch 12-16): 5 concepts max, 3-4 lessons
- Intermediate (Ch 17-23): 7 concepts max, 4-5 lessons
- Advanced (Ch 24-29): 10 concepts max, 5-6 lessons

---

### Rule 5: MINIMAL FILES

**Never create:**
- âŒ index.md, _templates/, _assets/, _code-examples/, lesson-template.md, capstone-rubric.md

---

### Rule 6: TROUBLESHOOTING IS AI PARTNERSHIP

**Real-world context:** In an AI-native world, students will encounter errors (installation, syntax, environment issues). Rather than detailed troubleshooting in every chapter, teach students to ASK their AI assistant.

**Application in chapters:**
- **Installation/Setup chapters**: Include prompt like: `"I tried to install Python but got this error: [error]. What does this mean and how do I fix it?"`
- **Execution chapters**: Include prompt like: `"My program runs but gives this output. Is this correct? Why?"`
- **Advanced chapters**: Include prompt like: `"I'm getting a TypeError. Walk me through what went wrong."`

**Why this works:**
- âœ… Teaches resilience: Errors are information to be understood, not obstacles
- âœ… Builds partnership: AI becomes problem-solving collaborator, not just code generator
- âœ… Scales with complexity: Works for simple errors (Python not found) to complex errors (type mismatches)
- âœ… Honors reality: Professional developers ask AI for error help constantly

**Example (from Chapter 13, Lesson 2):**
```markdown
### Prompt 2: Troubleshoot Installation Errors
\`\`\`
I tried to install Python but got this error: [describe your error].
What does this mean and how do I fix it?
\`\`\`

**Expected outcome**: AI explains the error and provides step-by-step fixing instructions.
```

This single prompt replaces 10 pages of platform-specific troubleshooting guides that become outdated.

---

### Rule 7: GRADUATED TEACHING PATTERN (Constitution Principle 13)

**Apply the three-tier teaching approach from the constitution:**

**Tier 1 - Foundational Concepts** (Book Teaches Directly):
- Stable, core concepts explained directly in book
- Direct explanation with analogies and examples
- Examples: What are variables? What is a loop? What are type hints?
- NO "Ask your AI: What is X?" for foundations
- Book provides clear, authoritative explanation first

**Tier 2 - Complex Syntax** (AI Companion Handles):
- Complex syntax patterns AI handles (student directs, AI executes)
- Student specifies WHAT they want, AI handles HOW
- Examples: Decorators, context managers, complex regex, async/await patterns
- "Tell your AI: Create X with these requirements..."
- Student learns strategy and intent, not memorization of syntax

**Tier 3 - Scaling Operations** (AI Orchestration):
- Operations involving 10+ items or multi-file workflows
- Student orchestrates strategy, AI manages tactical execution
- Examples: Setting up 10 test environments, batch file conversions, project scaffolding
- "Tell your AI: Set up 10 X with Y configuration..."
- Student learns orchestration and supervision skills

**Application to Part 4 (Chapters 12-29)**:
- Primarily Tier 1 (foundations) and Tier 2 (applied syntax)
- Tier 3 introduced gradually in advanced chapters (24-29)
- Balance: Book teaches concepts, AI handles complexity, student directs strategy

---

### Rule 8: STANDARDIZED "TRY WITH AI" FORMAT (End-of-Lesson Closure)

**Every lesson MUST end with "Try With AI" section ONLY** following this exact structure (verified in Chapter 1 and Chapter 13):

```markdown
## Try With AI

Use your AI companion (Claude Code or Gemini CLI). [Brief context about what you're exploring].

### Prompt 1: [Descriptive Title - Recall/Understand]
\`\`\`
[Clear, concrete prompt asking about the concept]
\`\`\`

**Expected outcome**: [What student should understand after AI response]

### Prompt 2: [Descriptive Title - Apply]
\`\`\`
[Clear, concrete prompt asking about application or edge case]
\`\`\`

**Expected outcome**: [What student learns from this]

### Prompt 3: [Descriptive Title - Analyze/Evaluate]
\`\`\`
[Prompt encouraging deeper understanding or connection to real-world use]
\`\`\`

**Expected outcome**: [Connection to AIDD or professional practice]

### Prompt 4: [Descriptive Title - Synthesis/Create]
\`\`\`
[Synthesis prompt pulling together concepts from lesson]
\`\`\`

**Expected outcome**: [Integration of understanding + forward-looking insight]
```

**Critical requirements:**
- âœ… Exactly 4 prompts per lesson (progressive Bloom's levels: Remember â†’ Understand â†’ Apply â†’ Analyze/Synthesize)
- âœ… Prompts are CONCRETE and SPECIFIC (not "ask AI about X")
- âœ… Each prompt has explicit "Expected outcome" describing what student learns
- âœ… Prompts should include rubric-style validation ("Does this answer your spec?")
- âœ… No "Key Takeaways" or "Summary" sections after "Try With AI"
- âœ… "Try With AI" is the final substantive section (closure point)

**CRITICAL LESSON CLOSURE PATTERN** (Constitutional Mandate):

Lessons MUST end with "Try With AI" section ONLY. Prompt 4 provides cognitive closure.

**NEVER ADD after "Try With AI":**
- âŒ "Key Takeaways" or "Summary"
- âŒ "What's Next"
- âŒ "Completion Checklist" (even for capstone lessons)
- âŒ "Chapter Summary"
- âŒ Any other closure content

**WHY**: Try With AI Prompt 4 already provides reflection and synthesis. Additional sections create cognitive overload and violate Constitutional Rule 13. This was identified as a critical violation in Chapter 14 technical review.

**Why this matters:**
- Consistency across entire book (students know the format)
- Progressive prompts teach exploration, not memorization
- "Expected outcome" sets clear learning targets
- Validates understanding without artificial quizzes
- Prompt 4 synthesis provides natural closure

---

### Rule 9: AI-NATIVE COLEARNING PEDAGOGY (Throughout Lessons)

**CRITICAL**: Apply `ai-collaborate-teaching` skill throughout ALL lessons, not just at the end.

**CoLearning Structural Elements** (must appear throughout lesson content, NOT just "Try With AI" section):

#### ğŸ’¬ AI Colearning Prompt (Claude Code or Gemini CLI)
- **When**: After introducing foundational concepts
- **Purpose**: Encourage deeper conceptual understanding with AI as co-reasoning partner
- **Format**:
```markdown
#### ğŸ’¬ AI Colearning Prompt
> "Explain how [concept] works under the hood. Why did Python choose this design?"
```
- **Example**: "Explain how `for` loops work under the hood with iterators. Why does Python need both `for` and `while`?"

#### ğŸ“ Instructor Commentary: "From Syntax to Semantics"
- **When**: After code examples, before moving to next concept
- **Purpose**: Reframe learning goals (understanding > memorization)
- **Key Mantra**: "Syntax is cheap â€” semantics is gold"
- **Format**:
```markdown
#### ğŸ“ Instructor Commentary
> In AI-native development, you don't memorize operator precedenceâ€”you understand when arithmetic matters and ask AI when confused. Syntax is cheap; understanding is gold.
```
- **Example**: "In AI-driven development, you don't memorize all 47 string methodsâ€”you understand what strings DO and ask AI: 'How do I format this string?'"

#### ğŸš€ CoLearning Challenge
- **When**: After explaining a concept, before moving to practice
- **Purpose**: Practice specification-driven thinking WITH AI collaboration
- **Pattern**: Specification â†’ AI Generation â†’ Explanation â†’ Understanding
- **Format**:
```markdown
#### ğŸš€ CoLearning Challenge

Ask your AI Co-Teacher:
> "Generate a function that calculates factorial using recursion. Then explain how recursion works step-by-step, including the call stack."

**Expected Outcome**: You'll understand recursion conceptually (not just syntax), see how AI generates code from specifications, and learn to validate AI output.
```
- **Example**: "Ask your AI: Generate a `for` loop that prints a multiplication table for 7. Then explain how `range()` works and why we use it instead of manual counting."

#### âœ¨ Teaching Tip
- **When**: Throughout lesson, when showing how to use Claude Code/Gemini CLI effectively
- **Purpose**: Build AI tool literacy and effective collaboration patterns
- **Format**:
```markdown
#### âœ¨ Teaching Tip
> Use Claude Code to explore edge cases: "What happens if I divide by zero? Show me the error and explain what ZeroDivisionError means."
```
- **Example**: "Use your AI tool to explore operator precedence: 'Evaluate this step-by-step: 2 + 3 * 4. Show me the evaluation order.'"

**Placement Guidelines by Proficiency Level**:

- **A1-A2 (Beginner)**:
  - 1-2 ğŸ’¬ prompts per lesson (foundational concepts only)
  - 2-3 ğŸ“ commentaries (emphasize understanding > syntax)
  - 1-2 ğŸš€ challenges (simple, guided)
  - 1-2 âœ¨ tips (basic tool usage)

- **A2-B1 (Intermediate)**:
  - 2-3 ğŸ’¬ prompts per lesson (concepts + edge cases)
  - 2-3 ğŸ“ commentaries (connect to design patterns)
  - 2-3 ğŸš€ challenges (specification-driven)
  - 2-3 âœ¨ tips (advanced tool usage)

- **B1-B2 (Advanced)**:
  - 3-4 ğŸ’¬ prompts per lesson (architectural exploration)
  - 3-4 ğŸ“ commentaries (professional reasoning)
  - 3-4 ğŸš€ challenges (complex specification-driven)
  - 2-3 âœ¨ tips (orchestration patterns)

**Tone Requirements for ALL Lessons**:
- âœ… Conversational (you, your, we)
- âœ… Exploration-focused (discover, explore, try)
- âœ… AI partnership emphasized (co-teacher, co-reasoning partner, pair-teacher)
- âŒ NOT documentation style
- âŒ NOT reference manual tone
- âŒ NOT traditional tutorial "here's how you do X" without AI collaboration context

**Critical Distinctions**:
- **CoLearning Elements** (throughout lesson): Conversational, exploration-focused, AI partnership throughout content
- **Try With AI Section** (end of lesson): Structured 4-prompt synthesis and reflection (closure point)

**Why This Matters**:
- Students learn WITH AI, not just USING AI
- AI positioned as intellectual partner, not autocomplete tool
- Builds critical thinking ("Why does this work?") not rote memorization
- Prepares for shipping era (professional AI-native development patterns)

**Validation**:
- technical-reviewer MUST check for CoLearning elements throughout
- Missing ğŸ’¬ğŸ“ğŸš€âœ¨ = CRITICAL VIOLATION (regeneration required)
- Documentation tone (not conversational) = MAJOR VIOLATION (revision required)

---

## ORCHESTRATED WORKFLOW (What Actually Happens)

When you run `/sp.python-chapter [N]`:

### PHASE 0: Intelligent Context Gathering (Adaptive + MCP-Enhanced)

**Intelligence-Driven Discovery** (not hardcoded questions):

1. **Read authoritative sources**:
   - Constitution: `.specify/memory/constitution.md` (target audience, philosophy, principles)
   - Chapter Index: `specs/book/chapter-index.md` (chapter title, part, prerequisite chapters)
   - Skills Library: `.claude/skills/` (available domain skills, especially ai-collaborate-teaching)
   - Existing Context: `context/part-4-python/` or `context/13_chap12_to_29_specs/` (if available)
   - **MCP Documentation**: Python.org official docs via context7 MCP server (if available)

2. **Load Python Documentation via MCP** (WHEN AVAILABLE):
   - Use MCP tools to fetch Python.org official docs (v3.14+)
   - Load relevant sections for the chapter (tutorial, stdlib types, functions, chapter-specific libraries)
   - Graceful fallback to cached context if MCP unavailable
   - Acknowledge documentation source in outputs

3. **Derive chapter intelligence**:
   - **Audience**: From constitution (Aspiring/Professional/Founders with graduated complexity)
   - **Part**: From chapter-index.md (chapter N â†’ Part X)
   - **Complexity Tier**: From chapter number range (12-16=beginner, 17-23=intermediate, 24-29=advanced)
   - **Prerequisite Knowledge**: All chapters 1 through N-1
   - **Chapter Title**: Exact title from chapter-index.md (THE ANCHOR)
   - **Learning Pattern**: AI-Native Learning (Part 4 appropriate, NOT formal SDD)

3. **Intelligently determine what to ask user** (context-adaptive):
   - IF context materials exist for this chapter â†’ Ask: "Use existing context or start fresh?"
   - IF chapter title is ambiguous/broad â†’ Ask: "What specific aspect should we emphasize?"
   - IF capstone vs foundational unclear â†’ Ask: "Should students BUILD something or learn concepts?"
   - IF multiple teaching approaches possible â†’ Ask: "Which pedagogical angle fits best?"

   **DO NOT ask**:
   - âŒ "Who is the audience?" (constitution already defines this)
   - âŒ "How many lessons?" (let intelligence determine based on scope)
   - âŒ "What CEFR level?" (derive from chapter number range automatically)

4. **Store derived intelligence** for next phases

**Apply Vertical Intelligence**: Constitution + Chapter Index + Skills â†’ Adaptive questions (not hardcoded forms).

**CRITICAL**: Do NOT create git branch yet. Branch creation happens in Phase 1 AFTER spec.md is created, ensuring branch name matches spec directory name.

---

### PHASE 1: Specification (Automated + Quality Gate)

```
â†’ Invoke: /sp.specify [chapter-context]
  â”œâ”€ Pass: chapter number, title, derived intelligence, context materials
  â”œâ”€ Apply: AI-Native Learning principles, cognitive load limits, teaching patterns
  â”œâ”€ Create: specs/part-4-chapter-[N]/spec.md
  â””â”€ Report: "Spec created."

â†’ Invoke: /sp.clarify (Quality Gate)
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md
  â”œâ”€ Identify: Underspecified areas, ambiguities, missing details
  â”œâ”€ Ask: Up to 5 targeted clarification questions
  â”œâ”€ Update: spec.md with answers encoded
  â””â”€ Report: "Spec clarified and updated."

â†’ Create Feature Branch (AFTER spec exists)
  â”œâ”€ Derive branch name from spec directory (e.g., specs/part-4-chapter-15/ â†’ part-4-chapter-15)
  â”œâ”€ Check if already on correct branch:
  â”‚   IF current branch == main â†’ Create new branch matching spec directory
  â”‚   IF current branch matches spec directory â†’ Stay on it
  â”‚   IF current branch != spec directory â†’ Warn and ask user to switch
  â”œâ”€ Execute: git checkout -b [spec-directory-name] (only if on main)
  â””â”€ Report: "âœ… Branch created: [branch-name]" or "â„¹ï¸  Already on branch: [branch-name]"

WAIT: User reviews spec.md
â†’ User confirms: "âœ… Spec approved" or provides feedback
  â”œâ”€ If feedback: Update spec.md iteratively (may re-run /sp.clarify)
  â””â”€ If approved: Continue to PHASE 2
```

**What /sp.specify receives:**
- Chapter title (anchor from chapter-index.md)
- User's audience answer (determines complexity tier: A1/A2/B1)
- User's scope answer (limits concepts to 5/7/10)
- User's outcome answer (real thing students will build)
- Context materials (extracted pedagogically)
- AI-Native Learning pattern (describe intent â†’ explore â†’ validate â†’ learn from errors)
- Teaching pattern template (What it is â†’ Code â†’ Try â†’ Why it matters)
- Cognitive load limits (max 5 for beginner, 7 for intermediate, 10 for advanced)

---

### PHASE 2: Planning (Automated + ADR Gate)

```
â†’ Invoke: /sp.plan [spec-context]
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md (clarified)
  â”œâ”€ Apply: Lesson progression, CEFR proficiency levels, AI prompts, skills-proficiency-mapper
  â”œâ”€ Create: specs/part-4-chapter-[N]/plan.md
  â””â”€ Report: "Plan created."

â†’ Invoke: /sp.adr (Architectural Decision Gate)
  â”œâ”€ Read: specs/part-4-chapter-[N]/plan.md
  â”œâ”€ Detect: Architecturally significant decisions (lesson structure, pedagogical approaches, tech choices)
  â”œâ”€ Suggest: "ğŸ“‹ Architectural decision detected: [X]. Document with /sp.adr [title]?"
  â”œâ”€ Wait: User consent to create ADR (never auto-create)
  â”œâ”€ Create: history/adr/[NNN]-[decision-title].md (if user approves)
  â””â”€ Report: "ADR created and linked to plan." OR "ADR suggestion noted."

WAIT: User reviews plan.md (+ any ADRs)
â†’ User confirms: "âœ… Plan approved" or provides feedback
  â”œâ”€ If feedback: Update plan.md iteratively
  â””â”€ If approved: Continue to PHASE 3
```

**What /sp.plan receives:**
- Approved spec.md (learning objectives, concepts, success criteria)
- Chapter scope (what fits this chapter, what doesn't)
- AI-Native Learning pattern (Describe intent â†’ Explore â†’ Validate â†’ Learn from errors)
- Proficiency expectations (CEFR A1/A2/B1 levels)
- Real outcome students will build
- Skills proficiency mapper for CEFR validation and cognitive load checks

---

### PHASE 3: Tasks (Automated + Analysis Gate)

```
â†’ Invoke: /sp.tasks [spec+plan-context]
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md + plan.md
  â”œâ”€ Apply: Acceptance criteria, validation steps, implementation checklist
  â”œâ”€ Create: specs/part-4-chapter-[N]/tasks.md
  â””â”€ Report: "Tasks created."

â†’ Invoke: /sp.analyze (Cross-Artifact Consistency Gate)
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md + plan.md + tasks.md
  â”œâ”€ Validate: Cross-artifact consistency (spec â†” plan â†” tasks alignment)
  â”œâ”€ Check: Learning objectives â†’ lessons â†’ tasks traceability
  â”œâ”€ Detect: Missing tasks, orphaned objectives, scope drift, conflicts
  â”œâ”€ Report: Consistency issues (critical/major/minor) with recommendations
  â””â”€ Output: analysis-report.md with findings

WAIT: User reviews tasks.md + analysis report
â†’ User confirms: "âœ… Tasks approved" or provides feedback
  â”œâ”€ If critical issues: Must fix before proceeding
  â”œâ”€ If major issues: Should fix (user decision)
  â”œâ”€ If minor issues: Nice to fix (user decision)
  â””â”€ If approved: Continue to PHASE 4
```

**What /sp.tasks receives:**
- Approved spec.md + plan.md (complete design)
- Learning objectives (what success looks like)
- Lessons (what needs to be implemented)
- Acceptance criteria (how to validate)

---

### PHASE 4: Implementation (Automated + Technical Review Gate)

```
â†’ Invoke: /sp.implement [chapter-slug]
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md + plan.md + tasks.md (all approved)
  â”œâ”€ Strategy: Parallel team approach (Lessons 1-4 parallel, Lesson 5 sequential capstone)
  â”œâ”€ Invoke: lesson-writer subagent (per lesson) WITH EXPLICIT COLEARNING INSTRUCTIONS
  â”œâ”€ Pass to lesson-writer:
  â”‚   CRITICAL INSTRUCTIONS FOR lesson-writer:
  â”‚
  â”‚   Apply these domain skills IN THIS ORDER:
  â”‚   1. ai-collaborate-teaching (CoLearning pedagogy THROUGHOUT lesson)
  â”‚   2. learning-objectives (aligned with CEFR proficiency levels)
  â”‚   3. concept-scaffolding (graduated complexity)
  â”‚   4. code-example-generator (Python 3.14+, type hints)
  â”‚   5. exercise-designer (deliberate practice)
  â”‚
  â”‚   CoLearning Structural Elements (MUST appear throughout lesson):
  â”‚   - ğŸ’¬ AI Colearning Prompt: After foundational concepts, encourage AI exploration
  â”‚   - ğŸ“ Instructor Commentary: Emphasize "syntax cheap, semantics gold"
  â”‚   - ğŸš€ CoLearning Challenge: Practice specification-driven thinking with AI
  â”‚   - âœ¨ Teaching Tip: Build AI tool literacy and collaboration patterns
  â”‚
  â”‚   Tone Requirements:
  â”‚   - âœ… Conversational (you, your, we)
  â”‚   - âœ… Exploration-focused (discover, explore, try)
  â”‚   - âœ… AI partnership (co-teacher, pair-teacher)
  â”‚   - âŒ NOT documentation style
  â”‚   - âŒ NOT reference manual
  â”‚
  â”‚   Lesson Closure:
  â”‚   - âœ… ONLY "Try With AI" section at end (4 prompts, Bloom's progression)
  â”‚   - âŒ NO summaries, checklists, "what's next" after Try With AI
  â”‚
  â”‚   CRITICAL PEDAGOGICAL ORDERING RULES (MUST ENFORCE):
  â”‚
  â”‚   **Rule 1: NO FORWARD REFERENCES WITHIN CHAPTER**
  â”‚   - ONLY use concepts/methods/functions taught in PREVIOUS lessons of this chapter
  â”‚   - NEVER use concepts from CURRENT or FUTURE lessons as examples
  â”‚   - Example VIOLATION: Using .upper() method in Lesson 1 when string methods are taught in Lesson 2
  â”‚   - Example CORRECT: In Lesson 1, use only string creation, indexing, len(), +, * (concepts taught IN Lesson 1)
  â”‚
  â”‚   **Rule 2: INTRODUCE BEFORE USE**
  â”‚   - Every method, function, or concept MUST be introduced BEFORE first use
  â”‚   - Introduction means: explain what it is, what it does, why it matters
  â”‚   - Example VIOLATION: Using len() without explaining it's a built-in function
  â”‚   - Example CORRECT: "Python provides built-in functions like len() that work on many types. len() counts characters in a string."
  â”‚
  â”‚   **Rule 3: DISTINGUISH BUILT-INS FROM METHODS**
  â”‚   - Built-in functions (len, type, isinstance): Explain they're "Python's built-in tools"
  â”‚   - Methods (.upper, .split, .join): Explain they're "actions strings can do"
  â”‚   - Always clarify: "len() is a built-in function, not a string method"
  â”‚
  â”‚   **Rule 4: CONCEPT PREREQUISITE VALIDATION**
  â”‚   Before writing any code example, ask:
  â”‚   - "Have all concepts in this example been taught in THIS lesson or PRIOR lessons?"
  â”‚   - "Do students have the prerequisite knowledge to understand this?"
  â”‚   - "Am I introducing anything new without explanation?"
  â”‚
  â”‚   **Rule 5: LESSON BOUNDARY ENFORCEMENT**
  â”‚   - Lesson 1 concepts ONLY available in Lesson 1
  â”‚   - Lesson 1 + Lesson 2 concepts available in Lesson 2
  â”‚   - Lesson 1-3 concepts available in Lesson 3
  â”‚   - Capstone: ALL chapter concepts available (but NO new concepts introduced)
  â”‚
  â”‚   [Full context: spec, plan, tasks, MCP docs, AI-Native Learning pattern, CEFR levels]
  â”œâ”€ Apply: AI-Native Learning pattern, CEFR levels, validation-first approach, CoLearning throughout
  â”œâ”€ Create: book-source/docs/04-Part-4-Python-Fundamentals/[N]-[chapter-name]/
  â”‚   â”œâ”€ readme.md
  â”‚   â”œâ”€ 01-[lesson-name].md (with ğŸ’¬ğŸ“ğŸš€âœ¨ throughout)
  â”‚   â”œâ”€ 02-[lesson-name].md (with ğŸ’¬ğŸ“ğŸš€âœ¨ throughout)
  â”‚   â”œâ”€ 03-[lesson-name].md (with ğŸ’¬ğŸ“ğŸš€âœ¨ throughout)
  â”‚   â”œâ”€ 04-[lesson-name].md (with ğŸ’¬ğŸ“ğŸš€âœ¨ throughout)
  â”‚   â””â”€ 05-[capstone-name].md (if applicable, with ğŸ’¬ğŸ“ğŸš€âœ¨ throughout)
  â””â”€ Report: "All lessons implemented with CoLearning pedagogy."

â†’ Invoke: technical-reviewer (Quality Gate)
  â”œâ”€ Read: All lesson files
  â”œâ”€ Validate: AI-Native CoLearning compliance (ğŸ’¬ğŸ“ğŸš€âœ¨ elements present throughout)
  â”œâ”€ Check: Conversational tone (not documentation style)
  â”œâ”€ Check: Lesson closure pattern (Try With AI ONLY, no summaries)
  â”œâ”€ Check: Part 4 language appropriateness, constitutional alignment
  â”œâ”€ Test: All code examples (Python 3.14+, modern type hints)
  â”œâ”€ **NEW: Check: Pedagogical Ordering Compliance (CRITICAL)**
  â”‚   â”œâ”€ Scan each lesson for forward references:
  â”‚   â”‚   - Lesson 1: Only uses concepts taught IN Lesson 1
  â”‚   â”‚   - Lesson 2: Only uses Lesson 1 + Lesson 2 concepts
  â”‚   â”‚   - Lesson N: Only uses Lessons 1 through N concepts
  â”‚   â”œâ”€ Verify all methods/functions introduced before use:
  â”‚   â”‚   - First use of any method MUST have explanation
  â”‚   â”‚   - Built-in functions (len, type, isinstance) explained as "Python's built-in tools"
  â”‚   â”‚   - String methods (.upper, .split) explained as "actions strings can do"
  â”‚   â”œâ”€ Flag violations:
  â”‚   â”‚   - CRITICAL: Using .upper() in Lesson 1 when methods taught in Lesson 2
  â”‚   â”‚   - CRITICAL: Using len() without explaining it's a built-in function
  â”‚   â”‚   - CRITICAL: Any concept used before introduction
  â”‚   â””â”€ Report: List all forward references and missing introductions
  â”œâ”€ Report: Validation report with PASS/CONDITIONAL PASS/FAIL
  â””â”€ Output: VALIDATION_REPORT_CHAPTER_[N].md

â†’ If CONDITIONAL PASS or FAIL:
  â”œâ”€ Apply fixes for critical issues (especially missing CoLearning elements)
  â”œâ”€ Re-run technical-reviewer
  â””â”€ Repeat until PASS

WAIT: User reviews lessons + validation report
â†’ User confirms: "âœ… Implementation approved"
  â””â”€ Proceed to PHASE 5 (finalization)
```

---

### PHASE 5: Finalization (Update Chapter Index)

```
â†’ Update: specs/book/chapter-index.md
  â”œâ”€ Find: Chapter N row in Part 4 table
  â”œâ”€ Update status: ğŸ“‹ Planned â†’ âœ… Implemented & Validated
  â”œâ”€ Update Implementation Status section at top:
  â”‚   â”œâ”€ Increment count: "X chapters" â†’ "X+1 chapters"
  â”‚   â””â”€ Add Chapter N status block with:
  â”‚       - Number of lessons implemented
  â”‚       - Technical review result (PASS + any critical issues fixed)
  â”‚       - Key features (AI-Native Learning, type hints, complexity tier)
  â”‚       - Date (YYYY-MM-DD format)
  â””â”€ Report: "Chapter index updated"

â†’ Optional: Create commit and PR
  â”œâ”€ User may request: "/sp.git.commit_pr" for automated git workflow
  â””â”€ Or: Manual commit with summary of chapter completion
```

**Chapter Index Update Pattern**:
```markdown
- âœ… **Implemented & Validated** (X chapters): Chapters 1-N, 30-33...
  - **Chapter N Status**: âœ… COMPLETE + VALIDATED (YYYY-MM-DD)
    - [lessons-count] lessons written with AI-Native Learning pattern
    - Technical review [PASS/CONDITIONAL PASS] ([critical-issues-count] critical issues fixed)
    - [key-features]: Type hints, "Try With AI" format, graduated complexity
```

---

## KEY PRINCIPLES (Always Applied)

### âœ… Take Context, Discuss, Make Chapters (The AI-Native Workflow)

**The "Shipping Era" Approach**:
1. **Take Context**: Load authoritative sources (constitution, chapter-index, MCP docs, existing materials)
2. **Discuss**: Engage with user to understand intent, clarify ambiguities, align on goals
3. **Make Chapters**: Generate production-ready content with built-in quality (CoLearning, validation, proficiency-mapping)

This workflow ensures:
- Context-aware generation (not generic templates)
- Human-AI collaboration (not autonomous generation)
- Quality built-in (not bolted on afterwards)
- Shipping-ready output (not drafts requiring major revision)

### âœ… AI-Native CoLearning Pedagogy First (Rule 9)
- Apply `ai-collaborate-teaching` skill THROUGHOUT lessons (not just end)
- CoLearning elements (ğŸ’¬ğŸ“ğŸš€âœ¨) positioned strategically in every lesson
- Conversational tone (you, your, we) - NOT documentation style
- AI positioned as co-reasoning partner, not autocomplete tool
- 40/40/20 balance: Foundation 40%, AI-Assisted 40%, Verification 20%
- "Syntax is cheap â€” semantics is gold" mantra reinforced

### âœ… MCP-Enhanced Intelligence (When Available)
- Load official Python documentation via context7 MCP server
- Fallback to cached context if MCP unavailable
- Reference docs for technical accuracy throughout workflow
- Documentation sources explicitly acknowledged

### âœ… AI-Native Learning First (Part 4 Appropriate)
- Apply AI-Native Learning pattern: describe intent â†’ explore â†’ validate â†’ learn from errors
- Reference AIDD principles from Chapters 1-11 for context (not formal methodology)
- Validation-first practice: "How will students test understanding?"
- AI partnership: "How will they use Claude Code/Gemini CLI as co-reasoning partners?"
- NO formal "specification writing" (that's Part 5+) - use "describe intent" framing

### âœ… No Forward References
- Zero mentions of Chapters 30+ (SDD taught later)
- No "Specification-Driven Development" terminology (use "AI-Native Learning")
- No concepts from future chapters
- Chapter title from `chapter-index.md` is the absolute anchor

### âœ… Honors User Intent
- User's audience choice = final decision (never override)
- User's scope answer = limits concepts (never expand)
- User's outcome answer = determines lessons (never modify)

### âœ… Ruthless Context Filtering
- Only extract context matching THIS chapter's title
- Skip concepts from future chapters (even if in materials)
- Skip advanced variations and tangential concepts

### âœ… Cognitive Load Limits
- Max 5 concepts for beginner (Ch 12-16)
- Max 7 concepts for intermediate (Ch 17-23)
- Max 10 concepts for advanced (Ch 24-29)

### âœ… Teaching Intelligence Preserved
- Every phase applies AI-Native CoLearning principles
- Every phase uses teaching patterns (Book â†’ AI Companion â†’ AI Orchestration)
- Every phase respects chapter boundaries
- Every phase validates against acceptance criteria
- Skills proficiency mapping applied in planning phase (CEFR levels, cognitive load)

---

## EXECUTION INSTRUCTIONS (For Claude Code)

**CRITICAL**: This is an EXECUTABLE orchestration prompt, not documentation. Claude Code must:
1. Follow this flow exactly, in this order
2. Automatically invoke downstream commands WITHOUT asking for approval first
3. Pass full context (AIDD principles, teaching patterns) to each command
4. Respect approval gates ONLY between phases (not before first invocation)

### THE ORCHESTRATED WORKFLOW (EXECUTABLE)

#### PHASE 0: Intelligent Context Discovery (Adaptive, NOT Hardcoded)

**1. Read Authoritative Sources** (Automatic, NO USER INTERACTION):
- Constitution (`.specify/memory/constitution.md`): audience, philosophy, principles
- Chapter index (`specs/book/chapter-index.md`): title, file name, part number
- Available skills (`.claude/skills/` directory)
- Existing context materials (`context/` directory, if any)

**2. Derive Chapter Intelligence** (Automatic computation):

```python
# From constitution (no need to ask user)
audience = "Aspiring/Professional/Founders (graduated complexity)"

# From chapter number (automatic tier assignment)
if 12 <= chapter_num <= 16:
    complexity_tier = "beginner"
    cefr_range = "A1-A2"
    max_concepts = 5
elif 17 <= chapter_num <= 23:
    complexity_tier = "intermediate"
    cefr_range = "A2-B1"
    max_concepts = 7
elif 24 <= chapter_num <= 29:
    complexity_tier = "advanced"
    cefr_range = "B1-B2"
    max_concepts = 10

# From chapter index (THE ANCHOR)
part_num = 4  # Chapters 12-29 are Part 4
prerequisites = f"Chapters 1-{chapter_num - 1}"
learning_pattern = "AI-Native Learning"  # Part 4 appropriate

# Store derived intelligence
chapter_intelligence = {
    "number": chapter_num,
    "title": chapter_title,  # FROM CHAPTER-INDEX.MD (authoritative)
    "part": part_num,
    "complexity_tier": complexity_tier,
    "cefr_range": cefr_range,
    "max_concepts_per_lesson": max_concepts,
    "prerequisites": prerequisites,
    "audience": audience,
    "learning_pattern": learning_pattern,
    "available_skills": skills
}
```

**3. Intelligently Determine What to Ask** (Context-adaptive):
- Only ask if genuinely ambiguous or requires human judgment
- Example triggers: existing context found, broad chapter title, unclear capstone vs conceptual
- Ask 0-3 targeted questions max (NOT hardcoded)
- Store user preferences in chapter intelligence

**Key Principle**: Intelligence derives from constitution + chapter-index + skills library. Only ask user when GENUINELY ambiguous or requires human creative judgment.

**CRITICAL**: Do NOT create git branch in Phase 0. Branch creation happens in Phase 1 AFTER spec.md is created (see Phase 1 workflow).

---

#### PHASE 1: Specification (Automated + Intelligent)

**THIS PHASE INVOKES `/sp.specify` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Ruthless filtering applied):
   - Gather user's 4 answers from PHASE 0
   - Extract materials from context directories (if available):
     - `context/13_chap12_to_29_specs/` (legacy structure)
     - `context/part-4-python/` (preferred structure)
     - Skip if no context available (spec from scratch is valid)
   - Apply ruthless filtering: Skip future chapters, skip advanced variations, skip tangential concepts
   - Embed AI-Native Learning principles in the context
   - Embed teaching patterns in the context (Book â†’ AI Companion â†’ AI Orchestration)
   - Embed cognitive load limits (5 for beginner, 7 for intermediate, 10 for advanced)

2. **Invoke /sp.specify with full context**:
   ```
   /sp.specify [chapter-slug]

   Write Chapter [N]: [Title] in Part [P]

   [Full AIDD context, user answers, teaching patterns, cognitive load rules, ruthlessly filtered context materials]
   ```

3. **Wait for /sp.specify completion**:
   - âœ… `specs/part-[P]-chapter-[N]/spec.md` is created
   - âœ… AIDD principles applied
   - âœ… Teaching patterns specified
   - âœ… Learning objectives aligned with evals

4. **Output approval checkpoint**:
   ```
   âœ… PHASE 1 COMPLETE: Specification Created

   ğŸ“‹ specs/part-[P]-chapter-[N]/spec.md

   Please review and confirm:
   - âœ… "Spec approved" to proceed to planning
   - ğŸ“ Feedback to revise specification
   - â“ Questions for clarification
   ```

5. **Wait for user approval**: Block here until user confirms "âœ… Spec approved" OR provides feedback

---

#### PHASE 2: Planning (Automated + Intelligent) - Triggered After Spec Approval

**THIS PHASE INVOKES `/sp.plan` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Read approved spec, add intelligence):
   - Read: `specs/part-[P]-chapter-[N]/spec.md` (the approved specification)
   - Extract: Learning objectives, key concepts, success criteria
   - Add: CEFR proficiency levels (A1/A2/B1 based on audience)
   - Add: Skills proficiency mapping (identify skills, assign CEFR levels, validate progression)
   - Add: Cognitive load validation (max concepts per lesson based on proficiency)
   - Add: Bloom's taxonomy alignment (cognitive complexity matching proficiency level)
   - Add: Lesson progression rules (foundational â†’ applied â†’ integration)
   - Add: AI prompts for each lesson (validation-first approach)
   - Add: Teaching pattern structure for every lesson (Book â†’ AI Companion â†’ AI Orchestration)

2. **Invoke /sp.plan with full context**:
   ```
   /sp.plan [chapter-slug]

   [Full context from spec, CEFR levels, lesson structure, AIDD teaching patterns]
   ```

3. **Wait for /sp.plan completion**:
   - âœ… `specs/part-[P]-chapter-[N]/plan.md` is created
   - âœ… Lessons broken down lesson-by-lesson
   - âœ… CEFR proficiency levels assigned
   - âœ… AI prompts specified

4. **Output approval checkpoint**:
   ```
   âœ… PHASE 2 COMPLETE: Plan Created

   ğŸ“‹ specs/part-[P]-chapter-[N]/plan.md

   Please review and confirm:
   - âœ… "Plan approved" to proceed to tasks
   - ğŸ“ Feedback to revise plan
   - â“ Questions for clarification
   ```

5. **Wait for user approval**: Block here until user confirms "âœ… Plan approved" OR provides feedback

---

#### PHASE 3: Tasks (Automated + Intelligent) - Triggered After Plan Approval

**THIS PHASE INVOKES `/sp.tasks` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Read approved spec + plan, add validation):
   - Read: `specs/part-[P]-chapter-[N]/spec.md` (learning objectives)
   - Read: `specs/part-[P]-chapter-[N]/plan.md` (lesson structure)
   - Add: Acceptance criteria for each lesson
   - Add: Validation steps (how to test understanding)
   - Add: Implementation checklist (content requirements)

2. **Invoke /sp.tasks with full context**:
   ```
   /sp.tasks [chapter-slug]

   [Full context from spec + plan, acceptance criteria, validation steps]
   ```

3. **Wait for /sp.tasks completion**:
   - âœ… `specs/part-[P]-chapter-[N]/tasks.md` is created
   - âœ… Implementation checklist defined
   - âœ… Validation steps specified

4. **Output completion report**:
   ```
   âœ… ALL DESIGN ARTIFACTS COMPLETE

   ğŸ“‹ specs/part-[P]-chapter-[N]/spec.md
   ğŸ“‹ specs/part-[P]-chapter-[N]/plan.md
   ğŸ“‹ specs/part-[P]-chapter-[N]/tasks.md
   ```

5. **Ask for next step**:
   ```
   Ready to implement?

   A) Implement with lesson-writer subagent
   B) Manual implementation (use tasks.md as checklist)
   C) Done for now (keep designs, skip implementation)
   ```

---

#### PHASE 4: Implementation (Optional) - Triggered Only If User Chooses A

**THIS PHASE INVOKES lesson-writer subagent IF AND ONLY IF USER CHOOSES OPTION A**

1. **Prepare context** (Read all 3 approved artifacts):
   - Read: `specs/part-[P]-chapter-[N]/spec.md`
   - Read: `specs/part-[P]-chapter-[N]/plan.md`
   - Read: `specs/part-[P]-chapter-[N]/tasks.md`
   - Add: AIDD teaching pattern (What it is â†’ Code â†’ Try â†’ Why it matters)
   - Add: CEFR levels for validation
   - Add: Validation-first approach (test understanding before moving on)

2. **Invoke lesson-writer subagent** (Only if user chose Option A):
   ```
   Task(
       subagent_type="lesson-writer",
       prompt=prepare_lesson_writer_prompt(
           spec, plan, tasks,
           aidd_teaching_pattern=True,
           cefr_levels=True,
           validation_first=True
       )
   )
   ```

3. **Wait for lesson-writer completion**:
   - âœ… `docs/part-[P]/chapter-[N]/{01,02,03,04}-lesson-*.md` created
   - âœ… Full AI-Native Learning methodology applied
   - âœ… AI partnership approach emphasized

4. **Invoke technical-reviewer** (Automatic validation):
   ```
   Task(
       subagent_type="technical-reviewer",
       prompt=f"""
       Validate Chapter {N}: {Title} with special focus on:

       **AI-Native Learning Principles**:
       - 4-step pattern applied (describe intent â†’ explore â†’ validate â†’ learn from errors)
       - AI positioned as co-reasoning partner, not coding assistant
       - Students directing AI, not passive learners

       **Part 4 Appropriate Language**:
       - NO "Specification-Driven Development" terminology (that's Part 5+)
       - Use "describe intent" not "write specifications"
       - AI-Native Learning framing, not professional SDD

       **Lesson Closure Pattern**:
       - ALL lessons end with "Try With AI" section ONLY
       - NO "Key Takeaways", "Summary", "Checklist" after Try With AI
       - Prompt 4 provides cognitive closure

       **Technical Accuracy**:
       - All code runs on Python 3.14+
       - Modern type hints throughout (list[int], dict[str, float], X | None)
       - No security issues, no hardcoded secrets

       **Constitutional Compliance**:
       - All 9 domain skills applied
       - Graduated teaching pattern followed
       - CEFR proficiency levels appropriate
       - Cognitive load within limits

       Output: Validation report with PASS/CONDITIONAL PASS/FAIL verdict
       """
   )
   ```

5. **Apply critical fixes** (if validation identifies issues):
   - Critical issues: MUST fix before proceeding
   - Major issues: SHOULD fix for quality
   - Minor issues: Optional improvements
   - Re-run technical-reviewer after fixes

6. **Final report**:
   ```
   âœ… CHAPTER [N] VALIDATED AND COMPLETE

   ğŸ“š Lessons created: docs/part-[P]/chapter-[N]/
   ğŸ“‹ Validation report: VALIDATION_REPORT_CHAPTER_[N].md

   Next steps:
   - Review validation report
   - Test lessons interactively
   - Prepare for publication
   - Commit to git
   ```

---

### CRITICAL EXECUTION RULES

1. **Sequential Invocation**: Phases execute in order (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4), never out of order
2. **Automatic Chaining**: Each phase automatically invokes the next command (no "ask user first")
3. **Approval Gates Only Between Phases**: User approves AFTER each phase completes, BEFORE next phase starts
4. **Context Preservation**: Each phase reads prior phase outputs and passes them forward
5. **Vertical Intelligence Embedded**: EVERY command invocation includes AIDD principles, teaching patterns, cognitive load rules
6. **Ruthless Filtering**: Materials from future chapters are SKIPPED, not extracted
7. **No User Override**: User intent (audience, scope, outcome) is NEVER overridden, only honored
8. **Feature Branch Creation**: Automatic checkout of feature branch in PHASE 0, before any other work
9. **All 3 Artifacts Required**: Spec, Plan, and Tasks must exist before implementation can proceed

---

## CRITICAL VALIDATION (Before Each Phase)

**PHASE 1 Validation** (before `/sp.specify`):
- âœ… Chapter number valid (12-29, Part 4 only)
- âœ… Chapter title matches `chapter-index.md`
- âœ… User's audience answer captured
- âœ… User's scope answer captured
- âœ… User's outcome answer captured
- âœ… Context will be ruthlessly filtered (skip future chapters)
- âœ… AI-Native Learning principles will be applied (NOT formal SDD)

**PHASE 2 Validation** (before `/sp.plan`):
- âœ… spec.md was created successfully
- âœ… Concept count â‰¤ tier limit (5/7/10 based on chapter range)
- âœ… No forward references (Chapters 30+ or SDD terminology)
- âœ… AI-Native Learning framing used (not formal SDD)
- âœ… Only Chapters 1-N are prerequisites
- âœ… Teaching pattern respected (Book â†’ AI Companion â†’ AI Orchestration)
- âœ… Skills proficiency mapping will be applied

**PHASE 3 Validation** (before `/sp.tasks`):
- âœ… plan.md was created successfully
- âœ… Lessons match spec's learning objectives
- âœ… Proficiency levels assigned (CEFR A1/A2/B1)
- âœ… Cognitive load validated (concepts per lesson within limits)
- âœ… AI prompts specified for each lesson (4 prompts, progressive)
- âœ… Validation points defined
- âœ… Lesson closure pattern specified (Try With AI ONLY)

**PHASE 4 Validation** (before lesson-writer):
- âœ… All 3 design files exist and are valid
- âœ… User chose implementation option
- âœ… Context filtered ruthlessly (no future chapters)
- âœ… AI-Native Learning principles embedded
- âœ… Graduated teaching pattern clear
- âœ… Ready for lesson content creation

**PHASE 4 Post-Implementation Validation** (technical-reviewer):
- âœ… All lessons implement AI-Native Learning pattern
- âœ… No SDD terminology used inappropriately
- âœ… Lesson closure pattern followed (Try With AI ONLY)
- âœ… Code quality validated (Python 3.14+, type hints)
- âœ… CEFR proficiency levels appropriate
- âœ… Constitutional compliance verified
- âœ… **NEW: Pedagogical Ordering Compliance (CRITICAL)**
  - No forward references within chapter (Lesson N only uses concepts from Lessons 1 to N)
  - All methods/functions introduced before first use
  - Built-in functions (len, type, isinstance) distinguished from methods (.upper, .split)
  - Every new concept has explicit introduction ("what it is, what it does, why it matters")

---

## WHAT GETS CREATED

**By End of PHASE 3** (mandatory):
```
specs/part-4-chapter-[N]/
  â”œâ”€â”€ spec.md       (What students learn + AI-Native Learning principles)
  â”œâ”€â”€ plan.md       (How we teach it, lesson-by-lesson + CEFR levels + skills mapping)
  â””â”€â”€ tasks.md      (Implementation checklist + validation)
```

**By End of PHASE 4** (if Option A chosen):
```
book-source/docs/04-Part-4-Python-Fundamentals/[N]-[chapter-name]/
  â”œâ”€â”€ readme.md                    (Chapter overview and navigation)
  â”œâ”€â”€ 01-[lesson-name].md          (Lesson 1)
  â”œâ”€â”€ 02-[lesson-name].md          (Lesson 2)
  â”œâ”€â”€ 03-[lesson-name].md          (Lesson 3)
  â”œâ”€â”€ 04-[lesson-name].md          (Lesson 4, or more based on plan)
  â””â”€â”€ 05-[capstone-name].md        (Optional capstone lesson)

VALIDATION_REPORT_CHAPTER_[N].md   (Technical review results)
```

---

## EXAMPLE EXECUTION (What User Sees)

```
> /sp.python-chapter 14

âº PHASE 0: Intelligent Context Discovery

ğŸ“– Reading authoritative sources...
  âœ“ Constitution: Target audience, philosophy, principles
  âœ“ Chapter Index: Chapter 14 = "Data Types" (Part 4)
  âœ“ Skills Library: 13 skills available
  âœ“ Context: Found existing materials in context/13_chap12_to_29_specs/

ğŸ§  Deriving chapter intelligence...
  âœ“ Chapter: 14 - "Data Types"
  âœ“ Part: 4 (Python Fundamentals)
  âœ“ Complexity Tier: Beginner (Chapters 12-16)
  âœ“ CEFR Range: A1-A2 (max 5 concepts/lesson)
  âœ“ Prerequisites: Chapters 1-13
  âœ“ Audience: Aspiring/Professional/Founders (graduated)
  âœ“ Learning Pattern: AI-Native Learning (NOT formal SDD)

ğŸ¤” Analyzing what needs clarification...
  â†’ Existing context materials found

  Q: Use existing context or start from scratch?
  [User: Use existing but adapt for AI-Native Learning emphasis]

  â†’ Chapter title "Data Types" is broad (could be 10+ topics)

  Q: Which specific data types should we emphasize?
  [User: Core types only - int, float, str, bool, None + type hints]

  â†’ Checking if capstone needed...

  Q: Should students BUILD something hands-on?
  [User: Yes - Interactive Type Explorer program]

âœ… Intelligence gathered (2 automatic derivations + 3 targeted questions). Moving to PHASE 1...

---

âº PHASE 1: Specification + Clarification Gate

Invoking: /sp.specify with derived intelligence...
Applying: AI-Native Learning principles, teaching patterns, cognitive load limits

[/sp.specify generates spec.md]

ğŸ“‹ Spec created: specs/part-4-chapter-14/spec.md

---

ğŸ” Running /sp.clarify to identify underspecified areas...

Analyzing spec for ambiguities...
  âœ“ Learning objectives: Clear and measurable
  âœ“ Prerequisites: Explicitly listed
  âš ï¸ Scope clarity: "Data Types" could mean 10+ topics
  âš ï¸ Code examples: Quantity not specified
  âš ï¸ Capstone: Build artifact not defined

Asking targeted clarifications (3 questions):

Q1: Which specific data types should we cover?
    (Found: int, float, str, bool, None, list, dict, tuple, set)
[User: Focus on core 5 only - int, float, str, bool, None. Collections are Chapter 18-19]

Q2: How many code examples per lesson?
[User: 3-6 examples per lesson, progressive complexity]

Q3: Should students BUILD a hands-on project?
[User: Yes - Interactive Type Explorer program demonstrating all core types]

âœ… Spec updated with clarifications

---

Please review the clarified specification:
- Does scope now match chapter title anchor "Data Types"?
- Are learning objectives achievable in 3.5-4 hours?
- Is capstone project appropriate for A1-A2 learners?

Reply with: "âœ… Spec approved" or provide feedback

[User: âœ… Spec approved]

âœ… Spec approved. Moving to PHASE 2...

---

âº PHASE 2: Planning + ADR Gate

Invoking: /sp.plan with approved spec...
Applying: Lesson progression, CEFR levels, AI prompts, skills-proficiency-mapper

[/sp.plan generates plan.md with skills metadata]

ğŸ“‹ Plan created: specs/part-4-chapter-14/plan.md
  - 5 lessons (4 foundational + 1 capstone)
  - CEFR: A2-B1 progression
  - Skills mapped to each lesson
  - 4 "Try With AI" prompts per lesson

---

ğŸ›ï¸ Running /sp.adr to detect architectural decisions...

Analyzing plan for significant decisions...
  âœ“ Lesson structure: Standard 4-prompt format (no ADR needed - established pattern)
  âœ“ CEFR progression: A2 â†’ A2-B1 â†’ B1 (standard tier)
  ğŸ“‹ Pedagogical approach detected: "Collections awareness in Lesson 4 vs deep dive later"

ğŸ“‹ Architectural decision detected:
   "Teaching collections as awareness-only in Chapter 14 vs comprehensive coverage"

   Rationale: Collections (list, dict, tuple, set) are complex (7+ concepts each).
   Chapter 14 scope = core types only. Deep dive deferred to Chapters 18-19.

   Decision: Lesson 4 teaches "what collections exist" + basic syntax only.
             No iteration, comprehension, or methods until Ch 18-19.

   Document this decision with ADR? (y/n)

[User: y]

Creating ADR: history/adr/014-collections-awareness-pattern.md
âœ… ADR created and linked to plan.md

---

Please review the plan + ADR:
- Are 5 lessons appropriate structure?
- Is collections awareness pattern justified?
- Are CEFR levels and cognitive load appropriate?

Reply with: "âœ… Plan approved" or provide feedback

[User: âœ… Plan approved]

âœ… Plan approved. Moving to PHASE 3...

---

âº PHASE 3: Tasks + Analysis Gate

Invoking: /sp.tasks with spec + plan...
Applying: Validation points, acceptance criteria, lesson structure

[/sp.tasks generates tasks.md]

ğŸ“‹ Tasks created: specs/part-4-chapter-14/tasks.md
  - 58 tasks across 8 phases
  - Organized by lesson (not user story)
  - Code testing tasks included

---

ğŸ”¬ Running /sp.analyze for cross-artifact consistency...

Analyzing spec â†” plan â†” tasks alignment...

**Traceability Matrix**:
  âœ“ Learning Objective 1 â†’ Lesson 1 â†’ Tasks T004-T010 âœ“
  âœ“ Learning Objective 2 â†’ Lesson 2 â†’ Tasks T011-T018 âœ“
  âœ“ Learning Objective 3 â†’ Lesson 3 â†’ Tasks T019-T026 âœ“
  âœ“ Learning Objective 4 â†’ Lesson 4 â†’ Tasks T027-T035 âœ“
  âœ“ Learning Objective 5 â†’ Lesson 5 â†’ Tasks T036-T043 âœ“

**Scope Validation**:
  âœ“ All spec concepts have corresponding lessons
  âœ“ All lessons have implementation tasks
  âœ“ No orphaned tasks (all map to spec/plan)

**Consistency Checks**:
  âœ“ CEFR levels consistent (spec A2-B1 = plan A2-B1 = tasks complexity)
  âœ“ Cognitive load respected (max 7 concepts per lesson in tasks)
  âœ“ Lesson closure pattern specified in tasks (Try With AI ONLY)
  âœ“ Code testing tasks for all examples

**Risk Analysis**:
  âš ï¸ MINOR: Lesson 5 (capstone) has 8 tasks - ensure not overwhelming
  âœ“ Mitigation: Capstone is integration (reuses L1-4 concepts, not 8 NEW concepts)

ğŸ“Š Analysis Report: All critical checks passed. 1 minor advisory noted.

---

Please review tasks.md + analysis report:
- Are 58 tasks reasonable for 5 lessons?
- Is spec â†’ plan â†’ tasks traceability clear?
- Any critical consistency issues?

Reply with: "âœ… Tasks approved" or provide feedback

[User: âœ… Tasks approved]

âœ… Tasks approved. Moving to PHASE 4...

---

âº PHASE 4: Implementation + Technical Review Gate

All design artifacts approved:
  âœ… specs/part-4-chapter-14/spec.md (clarified)
  âœ… specs/part-4-chapter-14/plan.md (ADR documented)
  âœ… specs/part-4-chapter-14/tasks.md (analyzed)

Invoking: /sp.implement 014-data-types

ğŸ“š Implementing lessons with parallel team strategy...
  â†’ Lessons 1-4: Parallel (4 lesson-writer subagents)
  â†’ Lesson 5: Sequential (capstone after L1-4 complete)

[lesson-writer subagents working...]

âœ… All 5 lessons created:
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/readme.md
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/01-variables-and-type-hints.md
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/02-integers-and-floats.md
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/03-strings-and-booleans.md
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/04-collections-awareness.md
  âœ“ book-source/docs/04-Part-4-Python-Fundamentals/14-data-types/05-type-explorer-capstone.md

---

ğŸ” Invoking technical-reviewer for validation...

Validating AI-Native Learning compliance...
  âœ“ 4-step pattern applied (describe â†’ explore â†’ validate â†’ learn)
  âœ“ AI positioned as co-reasoning partner
  âœ“ Part 4 appropriate language (no SDD terminology)

Validating lesson closure pattern...
  âœ“ Lessons 1-4 end with "Try With AI" ONLY
  âš ï¸ Lesson 5: Has "Capstone Completion Checklist" after "Try With AI" (CRITICAL VIOLATION)

Validating code quality...
  âœ“ All 19 code examples run on Python 3.14+
  âœ“ Modern type hints throughout
  âœ“ No security issues

Validating constitutional compliance...
  âœ“ All 9 domain skills applied
  âœ“ Graduated teaching pattern followed
  âœ“ CEFR proficiency appropriate

ğŸ“‹ Verdict: CONDITIONAL PASS (1 critical issue)

---

âš ï¸ CRITICAL ISSUE DETECTED: Lesson 5 closure pattern violation

Applying automatic fix...
  â†’ Removing lines 559-575 (Capstone Completion Checklist section)
  â†’ Lesson now ends at line 555 with "Try With AI" section

âœ… Fix applied

---

ğŸ” Re-running technical-reviewer...

âœ… PASS: All requirements met
  - Lesson closure pattern compliant
  - AI-Native Learning principles excellent
  - Code quality 100% (19/19 examples pass)
  - Constitutional alignment verified

ğŸ“‹ Validation Report: VALIDATION_REPORT_CHAPTER_14.md

---

âœ… WORKFLOW COMPLETE

Chapter 14 production-ready:
  âœ… Specification (clarified with /sp.clarify)
  âœ… Planning (ADR documented with /sp.adr)
  âœ… Tasks (consistency validated with /sp.analyze)
  âœ… Implementation (5 lessons, 94 KB content)
  âœ… Technical Review (PASS after critical fix)

Files created: 6 lessons + 1 validation report
Quality gates: 4/4 passed (clarify, ADR, analyze, technical-review)

Next: Commit to git â†’ Create PR â†’ Publish
```

---

## CRITICAL SUCCESS FACTORS

1. **Automatic Invocation**: `/sp.specify`, `/sp.plan`, `/sp.tasks` must be invoked automatically via SlashCommand tool with full intelligence context

2. **Vertical Intelligence Preserved**: Every phase applies AIDD principles, teaching patterns, pedagogical design, and chapter boundary awareness

3. **Approval Gates**: User must explicitly approve each phase ("âœ… Approved") before proceeding to next

4. **Context Preservation**: Each phase receives full context from all previous phases + vertical intelligence

5. **Ruthless Filtering**: Context extraction must skip any concepts from future chapters, even if present in materials

6. **User Authority**: User's answers to 4 questions are final â€” never override with assumptions

7. **Compliance**: Every phase validates against acceptance criteria before proceeding

8. **Teaching Quality**: Intelligence flows through all 4 phases, not just documentation

---

## REFERENCES

- **Chapter Index**: `specs/book/chapter-index.md` (Part 4 Chapters: 12-29)
- **Constitution**: `.specify/memory/constitution.md` (AI-Native Learning principles, domain skills, graduated teaching pattern)
- **Skills Library**: `.claude/skills/` (skills-proficiency-mapper, learning-objectives, concept-scaffolding, etc.)
- **Context Materials**:
  - `context/13_chap12_to_29_specs/` (legacy structure)
  - `context/part-4-python/` (preferred structure)

---

## ONE COMMAND. FULL INTELLIGENCE. COMPLETE WORKFLOW WITH QUALITY GATES.

Run `/sp.python-chapter [N]` and the system executes this opinionated workflow:

**PHASE 0: Intelligent Context Discovery**
âœ… Reads constitution + chapter-index + skills (automatic intelligence derivation)
âœ… Asks 0-3 targeted questions (only when genuinely ambiguous)

**PHASE 1: Specification + Clarification Gate**
âœ… `/sp.specify` â†’ Creates spec.md
âœ… `/sp.clarify` â†’ Identifies underspecified areas, asks up to 5 clarifications, updates spec
âœ… Human review â†’ Approval gate

**PHASE 2: Planning + ADR Gate**
âœ… `/sp.plan` â†’ Creates plan.md with CEFR levels, skills mapping
âœ… `/sp.adr` â†’ Detects architectural decisions, suggests documentation (waits for user consent)
âœ… Human review â†’ Approval gate

**PHASE 3: Tasks + Analysis Gate**
âœ… `/sp.tasks` â†’ Creates tasks.md with acceptance criteria
âœ… `/sp.analyze` â†’ Cross-artifact consistency check (spec â†” plan â†” tasks alignment)
âœ… Human review â†’ Approval gate

**PHASE 4: Implementation + Technical Review Gate**
âœ… `/sp.implement` â†’ lesson-writer creates all lessons (parallel + sequential strategy)
âœ… `technical-reviewer` â†’ Validates AI-Native Learning compliance, code quality, lesson closure
âœ… Auto-fix critical issues â†’ Re-validate until PASS
âœ… Human review â†’ Final approval

**Result: High-quality, AI-Native Learning-centered Python chapters with built-in quality assurance.**

---

**Note**: For PHR (Prompt History Record) creation after command completion, see constitution for instructions.
