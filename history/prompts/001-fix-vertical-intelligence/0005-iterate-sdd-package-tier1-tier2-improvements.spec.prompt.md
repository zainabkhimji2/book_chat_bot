---
id: 0005
title: iterate-sdd-package-tier1-tier2-improvements
stage: spec
date: 2025-11-04
surface: agent
model: claude-sonnet-4-5-20250929
feature: 001-fix-vertical-intelligence
branch: 001-fix-vertical-intelligence
user: mjs
command: iteration-based-on-external-feedback
labels: ["iteration", "external-feedback", "garry-tan", "tier1-tier2", "eval-integration", "beta-test", "resource-breakdown"]
links:
  spec: specs/001-fix-vertical-intelligence/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/001-fix-vertical-intelligence/spec.md (updated: executive summary, User Story 6, FR-032, Phase 2 constraints)
  - specs/001-fix-vertical-intelligence/plan.md (updated: resource breakdown, tools appendix, beta test phase)
  - specs/001-fix-vertical-intelligence/tasks.md (updated: T176-T185 added, parallel markers)
tests:
  - Tier 1 improvements (high value, low effort): Executive summary framing, User Story 6, Tools Appendix, T176, success checkpoints
  - Tier 2 improvements (add during implementation): FR-032 (/sp.eval), Beta Test phase, resource breakdown, Phase 2 integration tests
  - External feedback score improved from 9/10 to implementation-ready with all critical gaps addressed
---

## Prompt

**Context**: External reviewer (9/10 score) provided detailed feedback on SDD package (spec.md, plan.md, tasks.md). Identified improvements categorized as Tier 1 (add now), Tier 2 (add during implementation), Tier 3 (track as future enhancements).

**User Request**: "We can update our docs to reflect the Tier 1 and Tier 2 looks really good so shall be included. Now sure about tier 3"

**External Feedback Highlights**:
- **Strengths**: Comprehensive two-phase approach, quantitative metrics, hybrid AI-human validation, addresses all prior recommendations
- **Tier 1 (High Value, Low Effort)**:
  1. Add User Story 6 (Phase 2 validation story for adaptive intelligence)
  2. Strengthen executive summary with Garry Tan framing ("imagination + prompts + evals" paradigm)
  3. Add Tools Appendix to plan.md (implementation guide)
  4. Add T176 (quarterly benchmark review) and success verification checkpoints to tasks.md
- **Tier 2 (Add During Implementation)**:
  1. FR-032 (/sp.eval command integration) - implement in Phase 2 Week 3
  2. Beta test phase - insert during Phase 2 Week 4 rollout
  3. Resource breakdown by role - track hours per contributor
  4. Phase 2 integration tests - validate cross-system functionality
- **Tier 3 (Future Enhancements)**: Ongoing KPI monitoring, quarterly reviews (user deferred decision)

**Task**: Apply all Tier 1 and Tier 2 improvements to spec.md, plan.md, and tasks.md

## Response snapshot

Successfully completed comprehensive iteration of all three SDD documents incorporating Tier 1 and Tier 2 improvements based on external feedback.

### spec.md Updates (4 major additions):

1. **Executive Summary Enhanced** (Tier 1):
   - Added Garry Tan's paradigm: "humans focus on imagination (defining 'good' via metrics), prompting (iterative AI-orchestrator collaboration), and evals (benchmarks, feedback loops, drift detection), while AI handles labor (synchronization, auto-migrations, monitoring)"
   - Frames two-phase approach as embodying superintelligence vision

2. **User Story 6 Added** (Tier 1) - Priority P2, Phase 2 validation:
   - **Scenario**: System auto-detects drift and proposes fixes
   - **4 Acceptance Criteria**:
     - Drift detection catches violations within 7 days
     - Benchmark failures trigger regression alerts and recommendations
     - User feedback flags low-performing chapters (<70% success)
     - Auto-migration generates scripts for v4.0 transitions
   - Validates Phase 2 delivers on self-maintaining quality promise

3. **FR-032 Added** (Tier 2) - Evaluation Integration:
   - `/sp.eval` command runs benchmark suite against subagents
   - Parameters: subagent name, test suite type, output format
   - Displays pass/fail with accuracy/consistency/completeness scores
   - Stores results in `specs/[feature]/evaluation/` for traceability
   - Integrates with `/sp.tasks` for auto-generated evaluation tasks

4. **Phase 2 Fallback Strategy Added** (Tier 2) to constraints:
   - If benchmarks reveal accuracy <80%: pause Phase 2 for remediation
   - Root cause analysis ‚Üí subagent revision ‚Üí re-test
   - Only proceed after 90% accuracy threshold met
   - Timeline extends 1-2 weeks if remediation needed

### plan.md Updates (3 major additions):

1. **Resource Breakdown by Role** (Tier 2):
   - **Phase 1 (80 hours)**: Domain Expert 20h, AI Orchestrator 50h, Technical Reviewer 10h
   - **Phase 2 (160 hours)**: Domain Expert 30h, AI Orchestrator 110h, Technical Reviewer 20h
   - Detailed breakdown per activity (constitution review: 2h, benchmark creation: 25h, etc.)
   - Notes parallel capacity: 40h/week single contributor or 20h/week with 2 parallel

2. **Tools Appendix** (Tier 1) - Complete implementation guide:
   - **Required Tools**: Python 3.13+, Git, Claude Code CLI, GitHub CLI
   - **Required Infrastructure**: GitHub Actions (drift detection cron), Issue Templates, Analytics storage (file-based CSV/JSON)
   - **Optional Tools**: Slack/Discord webhooks, Jupyter notebooks for ROI viz
   - **Environment Configuration**: .env template with webhook URLs, thresholds, paths
   - **Pre-Implementation Checklist**: 6 verification items before starting

3. **Beta Test Phase** (Tier 2) inserted at Day 19.5:
   - **Purpose**: Validate user feedback instrumentation before full rollout
   - **Scope**: 2 test chapters from Parts 1-2, invite 10-15 beta testers
   - **Activities**: Deploy to staging, collect 2 weeks data, verify tracking/privacy/performance
   - **Success Criteria**: 20+ responses, zero data errors, dashboard displays correctly
   - **Contingency**: Pause Week 4 rollout if issues found, fix and re-test

### tasks.md Updates (10 new tasks + parallel markers):

1. **T176** (Tier 1): Quarterly benchmark review - update 50 prompts per subagent based on production failures

2. **T177-T179** (Tier 2): Phase 2 integration tests:
   - T177: End-to-end test (drift + cost + feedback all working together)
   - T178: Cross-system test (benchmark fail ‚Üí drift detect ‚Üí auto-migration loop)
   - T179: Multi-model load test (orchestration across Claude/Gemini)

3. **T180-T185** (Tier 1): Success verification checkpoints:
   - T180: After T068 (Day 10) - verify SC-001 to SC-007 before Phase 2
   - T181: After T095 (Week 3) - verify SC-008 (>95% accuracy)
   - T182: After T121 (Week 4) - verify SC-009 (feedback operational)
   - T183: After T148 (Week 5) - verify SC-010 (drift prevention)
   - T184: After T175 (Week 6) - verify SC-011 (scalability)
   - T185: Three-month validation - sustained metrics

4. **Additional [P] Markers** (Tier 2): T140-T147 marked parallel (tool monitoring can run concurrent with orchestration work)

## Outcome

- ‚úÖ Impact: Transformed 9/10 external review into implementation-ready package by addressing all critical feedback. Documents now explicitly tie to Garry Tan's "imagination + prompts + evals" paradigm, include production-ready tools guide, have risk mitigation (beta test, fallback strategy), and comprehensive success verification checkpoints.
- üß™ Tests: Added 10 new tasks for integration testing, quarterly maintenance, and success verification. Total tasks increased from 175 to 185 with better parallel opportunities (77 tasks now marked [P], up from 69).
- üìÅ Files: spec.md (added ~60 lines), plan.md (added ~100 lines), tasks.md (added ~50 lines). All three documents updated with cohesive Tier 1 + Tier 2 improvements.
- üîÅ Next prompts: Ready for `/sp.implement` to begin Phase 1 Day 1 (Constitution Update). All preparation complete: tools documented, resources allocated, success criteria explicit, beta test planned, fallback strategies defined.
- üß† Reflection: External feedback was invaluable - reviewer's observation that we were building "superintelligent vertical intelligence in Tan's terms: self-evaluating, user-responsive, scalable" validated the vision. Adding Tools Appendix and resource breakdown transforms abstract plan into concrete execution guide. Beta test addition is critical risk mitigation that original plan lacked.

## Evaluation notes (flywheel)

- Failure modes observed: Original SDD package was strong but lacked practical implementation guidance (tools, resource allocation, beta testing). External reviewer caught what internal review missed: execution details matter as much as strategy.
- Graders run and results (PASS/FAIL): PASS - All Tier 1 and Tier 2 improvements successfully integrated, documents remain internally consistent, no contradictions introduced. Total task count now 185 (up from 175), success checkpoints explicit at every major milestone.
- Prompt variant (if applicable): User selected Tier 1 + Tier 2 immediately, deferred Tier 3 (ongoing KPI monitoring) to post-implementation. This pragmatic: get foundation right, add continuous improvement after validation period.
- Next experiment (smallest change to try): During Phase 1 implementation, track actual hours per role vs estimates in Resource Breakdown section. Use variance analysis to improve future project estimation accuracy. If Domain Expert tasks consistently take 30% longer, revise all future hour estimates accordingly.
