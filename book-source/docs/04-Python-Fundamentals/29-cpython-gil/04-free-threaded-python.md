---
title: "Lesson 4: Free-Threaded Python (3.14+ Production Ready)"
chapter: 29
lesson: 4
duration_minutes: 120

# HIDDEN SKILLS METADATA (Institutional Integration Layer)
# Not visible to students; enables competency assessment and differentiation
skills:
  - name: "Free-Threading Paradigm Shift"
    proficiency_level: "B1-B2"
    category: "Technical"
    bloom_level: "Understand"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student explains what changed between Python 3.13 and 3.14; understands GIL is now optional; recognizes this as 30-year inflection point"

  - name: "Free-Threading Architecture"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Understand"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student describes per-thread state, lock-free data structures, biased locking; explains how free-threading enables parallelism"

  - name: "Three-Phase GIL Roadmap"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Remember"
    digcomp_area: "Information Literacy"
    measurable_at_this_level: "Student recalls Python 3.13 (experimental), 3.14 (production), 3.15+ (likely default); understands timeline and overhead progression"

  - name: "Free-Threading Installation & Setup"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student successfully installs free-threaded Python on their platform; verifies installation; creates venv on free-threaded base"

  - name: "GIL Availability Detection"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student writes code using `sys._is_gil_enabled()` to detect GIL status; handles all three return cases (None/True/False)"

  - name: "Free-Threading Performance Characteristics"
    proficiency_level: "B1-B2"
    category: "Technical"
    bloom_level: "Analyze"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student understands 5-10% single-threaded overhead; 2-10x multi-threaded gains; knows when overhead is justified; measures with real workloads"

  - name: "Runtime GIL Control"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Apply"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student uses PYTHON_GIL environment variable to control GIL at runtime; understands use cases (debugging, testing)"

  - name: "Thread Safety in Free-Threading"
    proficiency_level: "B1-B2"
    category: "Technical"
    bloom_level: "Understand"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student understands GIL removal does NOT eliminate need for locks; identifies race conditions; knows when explicit locking is required"

  - name: "Free-Threading Integration with Ecosystem"
    proficiency_level: "B1"
    category: "Technical"
    bloom_level: "Understand"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student understands free-threading's compatibility with asyncio, multiprocessing, C extensions; recognizes tool interactions"

  - name: "AI-Native Multi-Agent Architecture with Free-Threading"
    proficiency_level: "B2"
    category: "Technical"
    bloom_level: "Understand"
    digcomp_area: "Problem-Solving & Computational Thinking"
    measurable_at_this_level: "Student explains how free-threading enables parallel agent reasoning; understands foundation for Parts 10-14; connects to Ray/Kubernetes context"

learning_objectives:
  - objective: "Explain the paradigm shift from Python 3.13 to 3.14: the GIL is now optional"
    proficiency_level: "B1"
    bloom_level: "Understand"
    assessment_method: "Student articulates what changed, why it matters, and why it's the biggest Python change in 30 years"

  - objective: "Describe the three-phase GIL roadmap and performance implications at each phase"
    proficiency_level: "B1"
    bloom_level: "Remember"
    assessment_method: "Student recalls timeline and overhead percentages without reference"

  - objective: "Install and verify free-threaded Python on your platform"
    proficiency_level: "B1"
    bloom_level: "Apply"
    assessment_method: "Student successfully sets up free-threaded Python and confirms with detection code"

  - objective: "Write code that detects free-threading availability and status"
    proficiency_level: "B1"
    bloom_level: "Apply"
    assessment_method: "Student writes working code handling all three return cases from sys._is_gil_enabled()"

  - objective: "Analyze when free-threading overhead is justified by speedup"
    proficiency_level: "B1-B2"
    bloom_level: "Analyze"
    assessment_method: "Student performs benchmarking and explains when 5-10% overhead is worth the 2-10x gains"

  - objective: "Understand that thread safety remains programmer responsibility in free-threaded Python"
    proficiency_level: "B1-B2"
    bloom_level: "Understand"
    assessment_method: "Student identifies race conditions and explains when explicit locks are needed"

cognitive_load:
  new_concepts: 10
  assessment: "10 concepts (B1-B2 MAXIMUM for primary lesson): (1) free-threading paradigm shift, (2) per-thread GIL, (3) lock-free data structures, (4) three-phase roadmap, (5) installation methods, (6) detection via sys._is_gil_enabled, (7) PYTHON_GIL runtime control, (8) 5-10% overhead vs 2-10x gains, (9) thread safety remains critical, (10) AI-native multi-agent foundation. JUSTIFIED: This is the centerpiece lessonâ€”free-threading is revolutionary and requires deep understanding."

differentiation:
  extension_for_advanced: "For B2 students: Implement lock-free algorithms using atomic operations; explore how biased locking optimizes common patterns; connect to memory models and happens-before relationships"
  remedial_for_struggling: "For struggling students: Focus on paradigm shift (concept 1) and detection code (concept 6); use detection code as anchor; progress to performance characteristics only after detection is solid"

# Generation metadata
generated_by: "lesson-writer v3.0.0"
source_spec: "specs/part-4-chapter-29/spec.md (Lesson 4)"
created: "2025-11-09"
last_modified: "2025-11-09"
git_author: "Claude Code"
workflow: "/sp.implement"
version: "1.0.0"
---

# Lesson 4: Free-Threaded Python (3.14+ Production Ready)

## Opening: The Paradigm Shift

For 30 years, Python had a fundamental constraint: the Global Interpreter Lock prevented true parallel execution of Python threads. This wasn't a bugâ€”it was an architectural choice that protected memory safety and simplified the C API. It was also a defining limitation: multi-threaded Python could never achieve true parallelism on multi-core CPUs.

In October 2025, this changed forever.

Python 3.14 made free-threading production-ready. For the first time in Python's history, you can now write truly parallel multi-threaded Python code that scales across CPU cores. A 4-agent AI system running on a 4-core machine can achieve 2-4x performance gains instead of pseudo-concurrency. This is not a minor optimizationâ€”it's the biggest architectural change Python has experienced since its inception.

The GIL isn't removed (backward compatibility matters). It's now **optional**. You can choose to disable it.

This lesson is the centerpiece of Chapter 29 because free-threading transforms how you'll design multi-agent AI systems in Parts 10-14. You'll learn what changed, why it matters, how to use it, and when it's worth the 5-10% single-threaded overhead.

---

## Section 1: The Paradigm Shift (Biggest Change in 30 Years)

### What Changed Between Python 3.13 and 3.14?

**Python 3.13 (2024)**: Free-threading was experimental. You could build Python without the GIL, but it had 40% overhead on single-threaded code. Too slow for production.

**Python 3.14 (October 2025)**: Free-threading is officially supported and production-ready. Overhead dropped from 40% to 5-10%. The GIL is now optional, not fundamental.

This is the tipping point. For the first time in Python's history, you can choose: Do I want true parallelism on my CPU cores, or do I want to avoid the overhead?

### Why This is Revolutionary

**Traditional Python** (GIL always enabled):
- Threading provides **pseudo-parallelism** for I/O-bound work (OS releases GIL during I/O)
- Threading provides **NO parallelism** for CPU-bound work (GIL held continuously)
- Multi-agent reasoning systems were forced to use multiprocessing (high memory cost) or surrender parallelism entirely

**Free-Threaded Python** (GIL optional):
- Threading provides **TRUE parallelism** for CPU-bound work
- A 4-agent system on 4 cores achieves true parallel reasoning
- Memory overhead of multiprocessing disappears; threads share memory safely
- This makes multi-agent AI practical on a single machine

#### ðŸ’¬ AI Colearning Prompt
> "Explain what changed between Python 3.13 and 3.14 regarding the GIL. Why is this the biggest change in Python's history? What becomes possible now that wasn't before?"

This conversation will help you understand the historical context and the revolutionary implications.

### Not Removalâ€”Optionality

Critical distinction: **The GIL is not removed. It's optional.**

- In traditional Python builds (the default), the GIL is always enabled
- In free-threaded Python builds (opt-in), you can disable it
- Backward compatibility is preserved: existing code works on both builds
- You choose which version to install based on your workload

#### ðŸŽ“ Instructor Commentary
> Free-threading isn't just a featureâ€”it's a fundamental shift in how Python executes. Understanding this prepares you for the next 30 years of Python development, not just today's code. The GIL defined Python's capabilities for 30 years. Its optionality defines the next era.

---

## Section 2: The Three-Phase Roadmap

Free-threading isn't a flip-of-the-switch change. It's a deliberate, three-phase rollout designed to stabilize performance and let the ecosystem adapt.

### Phase 1: Python 3.13 (2024) â€” Experimental

**Status**: Experimental build available
**Overhead**: ~40% single-threaded (too high for production)
**Use Case**: Researchers, adventurous developers

At this phase, free-threading was a research project. You could build it from source, but nobody used it in production. The 40% overhead was prohibitive.

### Phase 2: Python 3.14 (October 2025) â€” Production Ready (WE ARE HERE)

**Status**: Officially supported, production-ready
**Overhead**: ~5-10% single-threaded (acceptable for most workloads)
**Multi-threaded Gains**: 2-10x on CPU-bound workloads
**Use Case**: Production AI systems, data processing, multi-agent reasoning

Python 3.14 is the inflection point. The overhead dropped dramatically (40% â†’ 5-10%), making free-threading practical for production. Official installers include free-threaded builds. This is where you choose to adopt (or not).

### Phase 3: Python 3.15+ (2026+) â€” Likely Default

**Status**: Expected (not yet confirmed)
**Overhead**: Likely further optimized
**Use Case**: Everyone (free-threading becomes default)

Eventually, free-threading will likely become the default build. Traditional GIL-only Python will become the opt-out option.

### Why Gradual?

Three reasons for this phased approach:

1. **Ecosystem Compatibility**: Third-party packages need time to validate compatibility. Some C extensions may need updates.
2. **Performance Stabilization**: The overhead improved 40% â†’ 5-10% in one year. More optimizations are likely in 3.15+.
3. **Developer Adoption**: Pushing too fast causes friction. Gradual rollout lets teams evaluate and migrate on their timeline.

#### ðŸš€ CoLearning Challenge

Ask your AI Co-Teacher:
> "Create a timeline comparison showing Python 3.13 vs 3.14 vs 3.15 (expected). What are the key differences in overhead, support status, and production readiness? Why does the gradual rollout matter for ecosystem adoption?"

**Expected Outcome**: You'll understand the strategic reasoning behind the three-phase approach and why 3.14 is the key inflection point.

---

## Section 3: How Free-Threading Works

### Traditional GIL Architecture

The traditional GIL is simple but restrictive: **one global mutex protects the entire interpreter state**.

```
[Thread 1] â†’ | GLOBAL LOCK | â† [Thread 2] â† [Thread 3] â† [Thread 4]
             (only one thread can hold it)
```

At any moment, only one thread can execute Python bytecode. When one thread is running, all others wait. The GIL releases during I/O (to allow pseudo-parallelism) but never during Python execution.

This design made CPython simple: no fine-grained locking, no complex synchronization. But it also meant CPU parallelism was impossible.

### Free-Threading: Per-Thread State

Free-threading fundamentally changes the architecture: **each thread gets its own state, eliminating the need for a global lock**.

```
[Thread 1] â†’ | Per-Thread State | â† [Thread 2] â†’ | Per-Thread State |
(independent execution)                            (independent execution)

[Thread 3] â†’ | Per-Thread State | â† [Thread 4] â†’ | Per-Thread State |
(independent execution)                            (independent execution)
```

Key insight: Threads don't compete for a global lock. Each thread maintains its own state. The interpreter executes threads in true parallel.

### Lock-Free Data Structures

But here's the challenge: built-in types (dict, list, set) are shared across threads. If Thread 1 modifies a dict while Thread 2 reads it, you need synchronization.

Free-threading uses **internal locks on shared objects** to prevent corruption:

- When Thread 1 modifies a dict, it acquires a lock on that specific dict
- Thread 2 can simultaneously modify a different dict (no lock contention)
- Built-in types are **thread-safe from CPython's perspective**

This is different from the GIL: the GIL is global and coarse-grained; these locks are fine-grained and per-object.

#### ðŸ’¬ AI Colearning Prompt
> "Explain per-thread state vs global interpreter state. How do lock-free data structures enable true parallelism? Draw a diagram showing the difference."

This will deepen your understanding of the architectural change.

### Biased Locking Optimization

Free-threading uses **biased locking**: if a thread accesses an object repeatedly, it "biases" the lock toward that thread to avoid repeated lock acquisitions.

Think of it like this: "This dict is usually accessed by Thread 1, so optimize for that case. If another thread tries to access it, revoke the bias and use slower but correct locking."

This optimization keeps the 5-10% overhead small. If every lock access required expensive synchronization, overhead would be much higher.

#### ðŸŽ“ Instructor Commentary
> Benchmarks show 2-10x gains, but YOUR workload may differ. Always measure. Professional developers don't trust marketing claimsâ€”they validate with real data. Free-threading's benefits depend on: (1) CPU-bound workloads, (2) multi-core hardware, (3) sufficient parallelism to justify overhead.

---

## Section 4: Installation and Setup

### macOS and Windows: Official Installers

The easiest path: use the official python.org installers, which now include a free-threaded option.

1. Visit [python.org](https://www.python.org/downloads/)
2. Download Python 3.14+ installer
3. During installation, check the "Free-threaded" option
4. Verify: `python --version` should show something like `Python 3.14.0 (free-threaded)`

That's it. The traditional GIL-only Python becomes the opt-in choice.

### Linux: Build from Source

On Linux, you typically build from source. Free-threading is a compile-time option:

```bash
./configure --disable-gil
make
make install
```

The `--disable-gil` flag disables the GIL. Verify with:

```bash
python -c "import sys; print(sys._is_gil_enabled())"
```

If it prints `False`, free-threading is active.

### Docker: The Convenient Option

The simplest path for reproducible environments:

```dockerfile
FROM python:3.14t  # The 't' means free-threaded

# Rest of your Dockerfile
```

Docker Hub now publishes `python:3.14t` images (t = free-threaded).

### Virtual Environments on Free-Threaded Python

Once you have free-threaded Python installed, create venvs normally:

```bash
/path/to/free-threaded-python -m venv myenv
source myenv/bin/activate
python --version
```

The venv inherits the free-threading setting from its base Python.

#### ðŸš€ CoLearning Challenge

Tell your AI Co-Teacher:
> "Help me install free-threaded Python on my machine. Verify it's working. Run `sys._is_gil_enabled()` to confirm. Then compare: what's different between free-threaded and traditional Python?"

**Expected Outcome**: You'll have a working free-threaded Python environment and understand the installation variations by platform.

---

## Section 5: Detecting Free-Threading at Runtime

You can't assume the Python interpreter running your code is free-threaded. A user might run your code on traditional Python, or a CI/CD pipeline might use a different build.

The solution: **detect free-threading at runtime** and handle both cases.

### The Detection API: `sys._is_gil_enabled()`

Python provides a special function (note the underscore prefixâ€”this is internal API):

```python
import sys

result: bool | None = sys._is_gil_enabled()
```

**Return values**:
- `True`: GIL is currently enabled (traditional Python build, or GIL forced on)
- `False`: GIL is disabled (free-threaded Python)
- `None`: This Python build doesn't support free-threading option (shouldn't happen in 3.14+)

### Example 2: Free-Threading Detection

Here's working code that detects free-threading and handles all cases:

**Specification Reference**: Detect GIL status and provide actionable information

**AI Prompt Used**: "Write a Python function that checks if free-threading is available and active. Handle all three return cases from sys._is_gil_enabled(). Include type hints and docstrings."

**Generated Code**:

```python
import sys
from typing import Optional

def check_free_threading() -> dict[str, bool | str]:
    """
    Detect free-threading availability and status.

    Returns a dictionary with:
    - 'build_supports_free_threading': True if this build has free-threading capability
    - 'gil_currently_enabled': True/False if GIL status known, 'N/A' if not
    - 'free_threading_active': True if free-threading is actually running

    Example:
        >>> result = check_free_threading()
        >>> print(result)
        {'build_supports_free_threading': True, 'gil_currently_enabled': False, 'free_threading_active': True}
    """
    gil_status: Optional[bool] = sys._is_gil_enabled()

    # Determine support: None means the build doesn't support free-threading options
    supports_free_threading: bool = gil_status is not None

    # Determine current GIL status
    gil_enabled_str: str = (
        str(gil_status) if gil_status is not None else "N/A"
    )

    # Check if free-threading is actively running
    free_threading_active: bool = gil_status == False  # False means GIL disabled

    return {
        "build_supports_free_threading": supports_free_threading,
        "gil_currently_enabled": gil_enabled_str,
        "free_threading_active": free_threading_active
    }


def recommend_concurrency_model() -> str:
    """
    Based on free-threading availability, recommend a concurrency approach.

    Returns a recommendation string.
    """
    result = check_free_threading()

    if not result["build_supports_free_threading"]:
        return "Upgrade to Python 3.14+ for free-threading support"

    if result["free_threading_active"]:
        return "Free-threading is active: use threading for CPU-bound parallelism"

    return "GIL is enabled: use asyncio for I/O-bound, multiprocessing for CPU-bound"


if __name__ == "__main__":
    detection = check_free_threading()
    print("Free-Threading Detection:")
    for key, value in detection.items():
        print(f"  {key}: {value}")

    print(f"\nRecommendation: {recommend_concurrency_model()}")
```

**Validation Steps**:
1. Run on free-threaded Python 3.14: Should print `'free_threading_active': True`
2. Run on traditional Python 3.14: Should print `'free_threading_active': False`
3. Run on Python 3.13: Should print `'build_supports_free_threading': False`

#### ðŸ’¬ AI Colearning Prompt
> "Walk me through this code line by line. Why does `gil_status is not None` check if the build supports free-threading? What does `sys._is_gil_enabled()` return in different Python builds?"

**Expected Outcome**: Deeper understanding of detection logic and handling edge cases.

#### âœ¨ Teaching Tip
> Use Claude Code to explore detection interactively: "Show me what `sys._is_gil_enabled()` returns on different Python builds. Help me write code that gracefully handles all three cases (None, True, False). Test it." This is how pros handle version compatibility.

---

## Section 6: Runtime Control via Environment Variable

Sometimes you want to force the GIL on or off at runtime, even if you're running free-threaded Python. This is useful for debugging, testing compatibility, and performance analysis.

### The PYTHON_GIL Environment Variable

Python 3.14+ respects the `PYTHON_GIL` environment variable:

```bash
# Force GIL enabled (even on free-threaded build)
export PYTHON_GIL=1
python my_script.py

# Force GIL disabled (even on free-threaded build, or if supported)
export PYTHON_GIL=0
python my_script.py

# Unset (use build default)
unset PYTHON_GIL
python my_script.py
```

### Use Cases

**Debugging**: Your free-threaded code has a race condition. Force GIL on to see if the race disappears:

```bash
PYTHON_GIL=1 python debug_script.py
```

**Testing Compatibility**: You want to ensure your code works on both GIL-enabled and free-threaded builds:

```bash
PYTHON_GIL=1 pytest tests/
PYTHON_GIL=0 pytest tests/
```

**Performance Comparison**: Benchmark your code with and without free-threading:

```bash
time PYTHON_GIL=1 python benchmark.py
time PYTHON_GIL=0 python benchmark.py
```

---

## Section 7: Performance Characteristics

This is where theory meets reality. Free-threading comes with tradeoffs.

### Single-Threaded Overhead: 5-10%

A single-threaded Python program running on free-threaded Python is 5-10% slower than traditional Python.

Why? Every object access now involves potential locking, biased locking revocation checks, and per-thread state management. It's small overhead, but it's real.

**Is it worth it?** Depends on your program:

- **Single-threaded program**: No. Use traditional Python.
- **Multi-threaded CPU-bound program**: Yes. The 5-10% overhead is tiny compared to 2-10x parallelism gains.

### Multi-Threaded Gains: 2-10x

On a 4-core machine, a free-threaded program with 4 threads solving a CPU-bound problem sees:

- **Ideal case** (perfect parallelism, no contention): ~4x speedup (1 thread baseline, 4 threads)
- **Real case** (some contention, biased locking revocation): 2-4x speedup (depends on workload)
- **Heavy contention case** (all threads access same objects): Close to 1x (no gain)

The speedup depends on:

1. **Parallelism**: How much work can actually run in parallel?
2. **Contention**: Do threads fight over the same locks?
3. **Overhead Amortization**: Is the work per thread substantial enough to justify the overhead?

### The Decision Framework

| Workload | Traditional Python | Free-Threaded Python | Multiprocessing |
|----------|------------------|----------------------|-----------------|
| Single-threaded | âœ… Optimal (0% overhead) | âŒ 5-10% slowdown | âŒ IPC overhead |
| Multi-threaded CPU-bound | âŒ No parallelism | âœ… 2-10x speedup | âœ… Parallelism, high memory |
| Multi-threaded I/O-bound | âœ… Works well | âœ… Works slightly better | âŒ Overkill |
| High contention | N/A | âŒ Lock contention | âœ… Process isolation |

### Example 6: Benchmark â€” Free-Threaded Python

**Specification Reference**: Demonstrate true parallelism with free-threaded Python on CPU-bound workload

**AI Prompt Used**: "Write a Python benchmark comparing traditional threading vs free-threaded Python on a CPU-bound task. Include type hints, proper timing, and clear output showing speedup."

**Generated Code**:

```python
import threading
import time
import sys
from typing import Optional

def cpu_task(n: int) -> int:
    """
    CPU-bound task: compute sum of square roots.

    This workload cannot release the GIL, so it demonstrates
    the difference between traditional threading (no parallelism)
    and free-threaded Python (true parallelism).
    """
    total = 0
    for i in range(n):
        total += int((i ** 2) ** 0.5)
    return total


def benchmark_threaded(n: int, num_threads: int) -> float | str:
    """
    Benchmark with threading (parallelism depends on free-threading).

    Returns elapsed time in seconds, or error message if not applicable.
    """
    gil_status: Optional[bool] = sys._is_gil_enabled()

    # If GIL is enabled, threading won't provide parallelism for CPU-bound work
    if gil_status == True:
        return "GIL is enabledâ€”threading provides no parallelism for CPU-bound work"

    if gil_status is None:
        return "Free-threading not supported in this build"

    # Free-threading is active (gil_status == False)
    threads: list[threading.Thread] = []
    start: float = time.time()

    for _ in range(num_threads):
        thread: threading.Thread = threading.Thread(target=cpu_task, args=(n,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    return time.time() - start


def benchmark_sequential(n: int, num_threads: int) -> float:
    """
    Baseline: run the same work sequentially (no threading).
    """
    start: float = time.time()

    for _ in range(num_threads):
        cpu_task(n)

    return time.time() - start


if __name__ == "__main__":
    # Configuration
    ITERATIONS = 15 ** 6  # Adjust based on your machine (bigger = longer)
    NUM_THREADS = 4

    print(f"Benchmarking CPU-bound work with {NUM_THREADS} threads")
    print(f"Iterations per thread: {ITERATIONS:,}")
    print()

    # Check free-threading status
    gil_status = sys._is_gil_enabled()
    if gil_status == True:
        print("GIL is ENABLED (traditional Python)")
        print("Threading will NOT provide parallelism for CPU-bound work\n")
    elif gil_status == False:
        print("GIL is DISABLED (free-threaded Python)")
        print("Threading WILL provide parallelism for CPU-bound work\n")

    # Baseline
    seq_time = benchmark_sequential(ITERATIONS, NUM_THREADS)
    print(f"Sequential (baseline):      {seq_time:.2f}s")

    # Threading
    thread_time = benchmark_threaded(ITERATIONS, NUM_THREADS)
    if isinstance(thread_time, str):
        print(f"Threaded:                  {thread_time}")
    else:
        speedup = seq_time / thread_time
        print(f"Threaded:                  {thread_time:.2f}s")
        print(f"Speedup:                   {speedup:.2f}x")
        print()

        if gil_status == False:
            print("âœ… Free-threading is active and providing parallelism!")
        else:
            print("âš ï¸  GIL is limiting parallelism")
```

**Validation Steps**:
1. Run on free-threaded Python 3.14: Should show ~2-4x speedup
2. Run on traditional Python 3.14: Should show ~1x speedup (no parallelism)
3. Adjust `ITERATIONS` if benchmark is too fast or slow

#### ðŸ’¬ AI Colearning Prompt
> "I'm concerned about 5-10% single-threaded overhead. When is this worth paying? Show me the math: at what thread count does the speedup justify the cost? Give real examples."

**Expected Outcome**: You'll understand the break-even analysis and when to choose free-threading.

#### ðŸŽ“ Instructor Commentary
> Don't believe claims about free-threading speedup. Ask your AI: "Generate a realistic benchmark for my AI workload. Measure traditional threading vs free-threaded vs multiprocessing. Show me the data." Measurement is your friend. Professional developers validate everything.

---

## Section 8: Thread Safety Still Matters

Here's the critical detail that many developers miss: **removing the GIL does NOT remove the need for explicit locking**.

### Built-In Types Are Safe, Sequences of Operations Aren't

Python's built-in types (dict, list, set) use internal locking in free-threaded Python. Individual operations are thread-safe:

```python
shared_dict: dict[str, int] = {}

# Thread 1
shared_dict["count"] = 1

# Thread 2
shared_dict["count"] = 2
```

Both assignments are safeâ€”the dict's internal lock prevents corruption.

**But this is NOT safe**:

```python
shared_dict: dict[str, int] = {}

# Thread 1: Check and update
if "count" in shared_dict:
    shared_dict["count"] += 1  # RACE CONDITION!

# Thread 2: Check and update
if "count" in shared_dict:
    shared_dict["count"] += 1  # RACE CONDITION!
```

Why? Between the `if` check and the update, another thread might modify the dict. The sequence of operations is not atomic.

With traditional GIL, this was still a potential race (though less likely). With free-threading, it's a guaranteed race if you don't use locks.

### When You Need Explicit Locks

Use `threading.Lock()` for multi-step operations:

```python
import threading

shared_dict: dict[str, int] = {"count": 0}
lock: threading.Lock = threading.Lock()

def increment() -> None:
    """Thread-safe increment using explicit lock."""
    with lock:  # Acquire lock
        shared_dict["count"] += 1
    # Lock released here
```

Now the entire increment operation is atomicâ€”no other thread can interfere.

#### ðŸ’¬ AI Colearning Prompt
> "The GIL is gone, but I still need locks? Why? Show me code that would race even with free-threading. How do I know when I need explicit locking?"

**Expected Outcome**: Crystal clear understanding that free-threading is not "locking is solved." It's "fine-grained locking is possible."

#### âœ¨ Teaching Tip
> Use Claude Code to identify races: "Here's my multi-threaded code. Could it race even with free-threading? Show me the problematic sequences. How would I fix it with locks?" This teaches professional thread-safety thinking.

---

## Section 9: Integration with Ecosystem Tools

Free-threading doesn't exist in isolation. It interacts with asyncio, multiprocessing, and C extensions.

### Asyncio and Free-Threading

**asyncio** is an event-driven concurrency library for I/O-bound work. It uses a single-threaded event loop.

**Compatibility**: Asyncio works fine with free-threaded Python. The event loop is still single-threaded. If you want to run multiple asyncio event loops in parallel, free-threading lets you do that:

```python
import asyncio
import threading

async def async_task(name: str) -> None:
    """An async task."""
    await asyncio.sleep(1)
    print(f"{name} done")

def run_event_loop(loop: asyncio.AbstractEventLoop) -> None:
    """Run an event loop in a thread."""
    asyncio.set_event_loop(loop)
    asyncio.run(async_task("Task"))

# With free-threading, multiple event loops run in true parallel
threads = [
    threading.Thread(target=run_event_loop, args=(asyncio.new_event_loop(),))
    for _ in range(4)
]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
```

With traditional GIL, this would be pseudo-parallelism (event loops would timeslice). With free-threading, true parallelism.

### Multiprocessing Still Has Its Place

**multiprocessing** creates separate Python processes, each with its own interpreter.

**Why still use it?** Three reasons:

1. **Process isolation**: If one process crashes, others continue. With threading, one thread's crash can kill the whole program.
2. **Resource limits**: Each process has isolated memory. You can control per-process resource limits.
3. **C extension isolation**: Some C extensions don't play well with threading. Separate processes isolate them.

**Trade-off**: Much higher memory overhead per process.

### C Extensions

C extensions can opt-in to free-threading support. If an extension releases the GIL during long operations, free-threaded Python benefits immediately.

If an extension doesn't support free-threading, it still works, but won't benefit from parallelism.

---

## Section 10: Connection to AI-Native Multi-Agent Systems

This is where theory meets practice and connects to the book's core focus.

### The Multi-Agent Reasoning Problem

In Part 10-14, you'll build multi-agent AI systems where multiple agents reason in parallel:

```
[Agent 1: Reason] â”€â”
[Agent 2: Reason] â”€â”¤â”€â†’ [Synthesize Results]
[Agent 3: Reason] â”€â”¤
[Agent 4: Reason] â”€â”˜
```

Each agent runs a reasoning task on a CPU core. Ideally, they should truly parallelize.

### Traditional Python's Limitation

With the GIL, this doesn't work:

```
Traditional Python (GIL enabled):
[Agent 1: Reason] â†’ GIL acquired
[Agent 2: Waiting] â†’ Waiting for GIL
[Agent 3: Waiting] â†’ Waiting for GIL
[Agent 4: Waiting] â†’ Waiting for GIL
```

All agents timeslice on a single core. No parallelism.

Workaround: Use multiprocessing. But now each agent needs its own Python process, separate interpreter, separate memory. Memory overhead becomes prohibitive with large models.

### Free-Threaded Python's Solution

With free-threading:

```
Free-Threaded Python:
[Agent 1: Reason] â†’ Core 1 (parallel)
[Agent 2: Reason] â†’ Core 2 (parallel)
[Agent 3: Reason] â†’ Core 3 (parallel)
[Agent 4: Reason] â†’ Core 4 (parallel)
```

All agents reason in true parallel on separate cores. Memory is shared (no duplication overhead). This is production-viable.

### The Speedup

On a 4-core machine:
- 1 agent: baseline
- 2 agents: ~1.9x speedup (almost 2x)
- 3 agents: ~2.8x speedup (almost 3x)
- 4 agents: ~3.7x speedup (almost 4x)

The 5-10% overhead is absorbed in the 2-4x parallelism gains.

#### ðŸš€ CoLearning Challenge

Tell your AI Co-Teacher:
> "I'm building a 4-agent AI system for my company. Free-threaded Python 3.14 vs multiprocessing vs asyncioâ€”which should I choose? Compare: complexity, memory, communication, deployment. What are the tradeoffs?"

**Expected Outcome**: You'll understand why free-threading is the right choice for multi-agent AI systems and when to use alternatives.

#### ðŸŽ“ Instructor Commentary
> This is where theory meets practice. Multi-agent systems are the book's core focus. Free-threading in 3.14 makes this practical on a single machine. In Parts 10-14 you'll scale this to Kubernetes and Ray. Today you learn the foundation.

---

## Try With AI

Now it's your turn to explore free-threaded Python and its revolutionary impact on multi-agent systems.

### Prompt 1: Recall â€” Understanding the Paradigm Shift

> "Explain in 2-3 sentences: What changed between Python 3.13 and 3.14 regarding the GIL? Then ask your AI: 'Did I capture the significance? Why is this such a big deal for Python development?'"

**What you'll learn**: Validate your understanding of the paradigm shift and its historical importance.

**Expected outcome**: Clear 2-3 sentence explanation that captures the revolutionary nature of optional free-threading.

**Time**: 3 minutes

---

### Prompt 2: Explain â€” Practical Implications

> "Ask your AI: 'Now that the GIL is optional, what becomes possible in Python that wasn't before? How does this change multi-threaded programming specifically? What do developers need to relearn about concurrency?'"

**What you'll learn**: Understand practical implications beyond the theory.

**Expected outcome**: AI explanation covering: (1) true CPU-bound parallelism now possible, (2) thread safety understanding deepens (no automatic safety), (3) performance implications, (4) decision-making framework.

**Time**: 3 minutes

---

### Prompt 3: Apply â€” Build Detection & Recommendation

> "Tell your AI: 'Help me write a Python script that: (1) detects if free-threading is available, (2) checks if GIL is currently enabled, (3) recommends the best concurrency approach for a given workload. Include error handling and test it on my Python build. Then explain your design choices.'"

**What you'll learn**: Write working free-threading detection code; create a reusable tool; understand why design choices matter.

**Expected outcome**: Runnable Python script that:
- Correctly identifies free-threading status
- Recommends concurrency approach based on status
- Includes error handling for edge cases
- Works on both free-threaded and traditional Python

**Time**: 5 minutes

---

### Prompt 4: Analyze â€” Connect to AI-Native Systems

> "Ask your AI: 'I'm building an AI system with 4 agents running reasoning tasks. How would free-threaded Python 3.14 improve this compared to traditional Python? Show me: (a) expected speedup on a 4-core machine, (b) code pattern changes, (c) memory overhead comparison vs multiprocessing, (d) deployment implications. Connect this to Ray and Kubernetes (Parts 11-14).'"

**What you'll learn**: See free-threading as foundation for production multi-agent architecture; understand production context; prepare for future lessons.

**Expected outcome**: AI response covering:
- Quantified speedup expectations (2-4x)
- Threading patterns vs multiprocessing patterns
- Memory overhead analysis
- Production deployment considerations
- Connection to Ray and Kubernetes

**Safety/Ethics Note**: Free-threading enables genuine parallelism, but thread safety remains your responsibility. Always use locks for multi-step operations on shared state. Validate with benchmarks on your hardware before production deployment.

**Time**: 9 minutes

---

**Total time**: 20 minutes

This "Try With AI" activity synthesizes all lesson concepts and connects free-threading to the production multi-agent systems you'll build in Parts 10-14.
